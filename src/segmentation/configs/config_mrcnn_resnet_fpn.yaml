defaults:
  - models: maskrcnn_fpn        # Loads configs/models/mask_rcnn.yaml
  - models/backbones: resnet_fpn     # Loads configs/models/backbones/resnet_fpn.yaml
  - datasets: skittlez       # Loads configs/datasets/skittlez.yaml
  - metrics: metrics        # Loads configs/metrics/metrics.yaml
  - transforms: transforms_skittlez # Loads configs/transforms/transforms_skittlez.yaml
  - _self_  # self reference last to allow for overrides

# overrides
# ----------------

models:
  backbones:
    _target_: segmentation.models.backbones.fpn.BackboneWithFPN
    # NOTE: FPN may add an extra LastLevelMaxPool layer to the last feature map
    # Hence, the output feature maps may have len(return_layers) + 1 maps
    # return layers specifies which feature maps to run through the FPN
    return_layers: {"p2": "p2", "p3": "p3"} 
    in_channels_list: [256, 512] # , 1024, 2048
    out_channels: ${backbone_out_channels}
    norm_layer: null
  min_size: 128 # images rescaled such that shorter size is min_size
  max_size: 128
  mask_roi_pool:
    _target_: segmentation.models.heads.poolers.MultiScaleRoIAlign
    featmap_names: ${output_features}
    output_size: 14 # ROI pooling output size
    sampling_ratio: 2 # grid subsampling 
    canonical_scale: 128
    canonical_level: 4 # levels are 2,3,4 i.e. 2^2, 2^3, 2^4 downsampling of input
  rpn_anchor_generator:
    _target_: segmentation.models.rpn.anchor_generator.AnchorGenerator
    sizes:
      - [16, 24, 32]
      - [32, 48, 64]
      # - [64, 96, 128]
    aspect_ratios:
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      # - [0.5, 1.0, 2.0]
    aspect_ratios_z:
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      # - [0.5, 1.0, 2.0]
  rpn_head: 
    _target_: segmentation.models.rpn.rpn_head.RPNHead
    in_channels: ${backbone_out_channels} 
    num_anchors: 27 # 3 sizes * 3 aspect ratios * 3 aspect ratios_z (TODO: make this dynamic)
  
  # NOTE: consider chaning the number of positive samples considered if the model is not learning
  #       and only predicts background
  # rpn_fg_iou_thresh: 0.5 # too high of a value will cause the model to not learn anything and always predict the background
  # rpn_bg_iou_thresh: 0.3
  # box_positive_fraction: 0.25 # positive fraction of samples in subsampling

datasets:
  batch_size: 16 # total batch size (batch_size per gpu * number of gpus) 
  database:
    _target_: segmentation.data.datasets.skittlez.Skittlez_Database
    training: true 
    distributed: true # set to true for distributed training (Ray or DDP)
    db_path: /clusterfs/nvme/segment_4d/final_pipeline_v3/db/segmentation_curation_test.db
    train_db_savedir: ${...train_db_savedir} # defer interpolation to main config
    selection_criteria: {"volume" : 8000}  # Minimum volume in pixels to accept instance
    force_create_db: false
    clean_up_db: false
    with_zarr: false
    with_tiff: true
    metadata: null
    batch_config:
      _target_: segmentation.data.data_utils.DataConfig
      z: 128 
      y: 128
      x: 128
      c: 3
      color_mode: MATCH  

transforms:
  transforms_list:
    - _target_: segmentation.data.transforms.transforms.Normalize
      # mean: ${dataset_mean}
      # std: ${dataset_std}
    - _target_: segmentation.data.transforms.transforms.Resize
      size: [128, 128, 128]

# ----------------

# Model type
network: maskrcnn

# Training paradigm
paradigm: segmentation.training.backend_segmentation.supervised

# Backbone 
backbone_target: segmentation.models.backbones.resnet.resnet50 # options: resnet50, resnet101, resnet152
backbone_out_channels: 256 # C5 = 2048 for ref.
output_features: ["p2", "p3"] # "p4", "pool" 

# Paths
outdir: /clusterfs/nvme/segment_4d/test_15
logdir: ${outdir}/logs

load_checkpointdir: null # specify if we want to load a checkpoint from a previous run that is not latest local run
checkpointdir: ${outdir}/checkpoints # specify where to save the checkpoints
checkpoint_update_interval: 100 # save checkpoint every N epochs

train_db_savedir: ${outdir}/db
db_path: /clusterfs/nvme/segment_4d/final_pipeline_v3/db/segmentation_curation_test.db

finetune: null

# Optimizer parameters
opt: adamw # TODO: to use fused lamb, install DeepSpeed with the DS_BUILD_FUSED_LAMB environment variable 
lr: 5e-5 # initial learning rate
wd: 5e-5 # initial weight decay
ld: null # optional layer decay
clip_grad: 0.5
ema: [0.998, 1.0]

# Training loop parameters
epochs: 2000
val_interval: 1
log_step: 100
val_log_step: 20
warmup: 25
cooldown: 50
fixedlr: false
dropout: 0.1
fixed_dropout_depth: false

# Quantization
amp: fp16  # ["no", "fp16", "bf16", "fp8"]

# Profiling
profile: false

# Compute resources
workers: 1 # number of worker nodes
gpu_workers: 4 #4 # number of gpus per worker
cpu_workers: 16 #16 # number of cpu cores per worker (num gpus per worker * number of cpu cores per gpu)
distributed_sampler: true # use distributed sampler for training (standard for Ray/DDP)
max_worker_heartbeat_timeout: 2 # in minutes (to prevent indefinite hang if one process fails)

# DeepSpeed config (Hydra will pass it to training loop)
deepspeed_config:
  fp16:
    enabled: true
    auto_cast: true
    loss_scale: 0 #  dynamic loss scaling (0 = auto)
    loss_scale_window: 1000 # number of good steps before increasing the scale again.
    hysteresis: 2 # number of overflows to wait before decreasing the loss scale.
    consecutive_hysteresis: true # if true, hysteresis must be consecutive (i.e., N overflows in a row)
    min_loss_scale: 1 # minimum loss scale to use
  bf16:
    enabled: false 
    auto_cast: false
  zero_optimization:
    stage: 3 # full ZeRO-3 with parameter + gradient + optimizer partitioning
    reduce_bucket_size: auto # controls how many elements are grouped before being reduced
    reduce_scatter: true # fused reduce + scatter ops
    contiguous_gradients: true # gradients allocated in a contiguous memory block
    overlap_comm: true # overlap communication with computation
    stage3_prefetch_bucket_size: auto # params to prefetch in advance
    stage3_param_persistence_threshold: auto # keep parameters in memory if theyâ€™re small and reused often
    stage3_max_live_parameters: 1e9 
    stage3_max_reuse_distance: 1e9
    stage3_gather_16bit_weights_on_model_save: true # reconstruct full weights (in FP16) during saving, rather than saving sharded pieces
    offload_optimizer: # disables offloading to CPU, everything stays on GPU
      device: none 
    offload_param:
      device: none
  tensorboard:
    enabled: true
    output_path: ${logdir}
    job_name: ${outdir}
  csv_monitor:
    enabled: true
    output_path: ${logdir}
    job_name: ${outdir}
  gradient_clipping: ${clip_grad}
  steps_per_print: 100
  gradient_accumulation_steps: 1 # number of steps to accumulate gradients before performing a backward/update pass
  train_batch_size: ${datasets.batch_size}
  zero_allow_untested_optimizer: true
  flops_profiler:
    enabled: true
    profile_step: 1
    module_depth: -1
    top_modules: 1
    detailed: true
    output_file: ${logdir}/flops_profiler.log

# Ray ScalingConfig
scaling_config:
  num_workers: null
  use_gpu: true
  resources_per_worker:
    CPU: null
    GPU: 1
  trainer_resources:
    CPU: 1

# Ray RunConfig
run_config:
  log_to_file: true
  storage_path: ${outdir}
  checkpoint_config:
    num_to_keep: 3
    checkpoint_score_attribute: loss
    checkpoint_score_order: min
  failure_config:
    max_failures: 0
  
# Torch Config
torch_config:
  timeout_s: 3600