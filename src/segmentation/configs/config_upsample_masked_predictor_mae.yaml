defaults:
  - models: encoder_decoder # Loads configs/models/encoder_decoder.yaml
  - models/preprocessors: preprocessor # Loads configs/models/preprocessors/preprocessor.yaml
  - models/backbones: masked_encoder_wrapper # Loads configs/models/backbones/masked_encoder_wrapper.yaml
  - models/heads: masked_predictor_wrapper # Loads configs/models/heads/masked_predictor_wrapper.yaml
  - models/mask_generators: mask_generator # Loads configs/models/mask_generators/mask_generator.yaml
  - models/patch_embeddings: patch_embedding # Loads configs/models/patch_embeddings/patch_embedding.yaml
  - losses: mae_loss # Loads configs/losses/mae_loss.yaml
  - datasets: upsample # Loads configs/datasets/upsample.yaml
  - transforms: transforms_upsample # Loads configs/transforms/transforms_upsample.yaml
  - optimizers: adamw # Loads configs/optimizers/adamw.yaml
  - schedulers: warmup_cosine_decay # Loads configs/schedulers/warmup_cosine_decay.yaml
  - metrics: metrics # Loads configs/metrics/metrics.yaml
  - deepspeed: deepspeed # Loads configs/deepspeed/deepspeed.yaml
  - clusters: local # Loads configs/clusters/slurm_multinode.yaml
  - _self_  # self reference last to allow for overrides

# overrides
# ----------------

clusters:
  batch_size:               1          # NOTE: batch size should divide im2col_step in deformable attn
  total_cpus:               4          # total number of cpus to use
  total_gpus:               1          # total number of gpus to use
  gpus_per_worker:          1          # number of gpus to use per node
  mem_per_worker:           31000      # memory per node
  cpus_per_worker:          4          # number of cpus to use per node  
  workers:                  1          # number of nodes to use 

models:
  masking: true # enable masking
  backbones:
    channel_predict: false
  heads:
    output_embed_dim: 8192
    channel_predict: false
  losses:
    patchify: true
  mask_generators:
    patchify_scheme: downsample_time
    time_downsample_pattern: [0,1,0,1]

datasets:
  # TODO: merge with layout parameter
  input_format: BTZYXC
  input_shape: 
      - 1 # B
      - 1 # T
      - 128 # Z
      - 128 # Y
      - 128 # X 
      - 2 # C
  channel_in: 2
  num_classes: 2
  databases:
    tile: true
    with_labels: true

    db_readpath: /clusterfs/nvme/segment_4d/testing2/db_tables_test9
    db_savepath: /clusterfs/nvme/segment_4d/testing2/db_tables_test9

    db_read_method: "feather"
    db_save_method: "feather"
    force_create_db: false

    # FIXME: here TCZYX but elsewhere TZYXC
    data_tile: [4, 2, 128, 128, 128]
    label_tile: [4, 128, 128, 128]

    label_spec:
      _target_: segmentation.data.databases.schema.DB_LabelSchema
      # table name based on database nomenclature
      table: "prepared"
      label_metadata:
        id: null
        server_folder: null
        output_folder: null
        z_start: null
        z_end: null
        y_start: null
        y_end: null
        x_start: null
        x_end: null

# ----------------

# Model type
network: mae

# Training paradigm
paradigm: segmentation.train.backend_segmentation.supervised

# Paths
outdir: /clusterfs/nvme/segment_4d/testing_2
logdir: ${outdir}/logs

load_checkpointdir: null # specify if we want to load a checkpoint from a previous run that is not latest local run
checkpointdir: ${outdir}/checkpoints # specify where to save the checkpoints
checkpoint_update_interval: 100 # save checkpoint every N epochs

val_interval: 1
log_step: 100 # step interval to show training loss in train loop
val_log_step: 20 # step interval to show loss in val loop