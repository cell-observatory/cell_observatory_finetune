defaults:
  - /models: maskrcnn_fpn        # Loads configs/models/maskrcnn_fpn.yaml
  - /models/backbones: hiera_fpn     # Loads configs/models/backbones/hiera_fpn.yaml
  - /datasets: skittlez_eval       # Loads configs/datasets/skittlez_eval.yaml
  - /metrics: metrics_skittlez_eval        # Loads configs/metrics/metrics_skittlez_eval.yaml
  - /transforms: transforms_skittlez_eval # Loads configs/transforms/transforms_skittlez_eval.yaml
  - _self_

# overrides
# ----------------

models:
  backbones:
    _target_: segmentation.models.backbones.fpn.BackboneWithFPN
    return_layers: {"p0": "p0", "p1": "p1", "p2": "p2"}
    in_channels_list: [96, 192, 384] # 768  (matches embed_dim * stage sequence)
    out_channels: ${backbone_out_channels}
    norm_layer: null
    backbone:
      _target_: ${backbone_target}

      input_size: [256, 256, 256]
      in_chans: 3
      embed_dim: 96  # initial embed dim
      num_heads: 1  # initial number of heads
      num_classes: 2

      # patch_embed -> stage 0 => 64x64x64, stage 1 => 32x32x32, stage 2 => 16x16x16 due to q-pooling [Optional Stage 3 => 8x8x8]
      stages: [2, 3, 16] # , 3 (may add stage 3 for 8x8x8)
      q_pool: 2 # 3 # number of q_pool stages
      q_stride: [2, 2, 2]
      mask_unit_size: [8, 8, 8]  # must divide q_stride ** (#stages-1)
      # mask_unit_attn: which stages use mask unit attention?
      mask_unit_attn: [True, True, False, False]

      dim_mul: 2.0 # 4 stages =>  embed_dim = (96, 192, 384, 768)
      head_mul: 2.0 # 4 stages => num_heads = (1, 2, 4, 8)

      patch_kernel: [7, 7, 7] # for conv patch_embed, 4x downsample
      patch_stride: [4, 4, 4] 
      patch_padding: [3, 3, 3]

      mlp_ratio: 4.0
      drop_path_rate: 0.0
      norm_layer: "LN"
      head_dropout: 0.0
      head_init_scale: 0.001
      sep_pos_embed: False
  mask_roi_pool:
    _target_: segmentation.models.heads.poolers.MultiScaleRoIAlign
    featmap_names: ${output_features}
    output_size: 14 # ROI pooling output size
    sampling_ratio: 2 # grid subsampling 
    canonical_scale: 256
    canonical_level: 4 # levels are 2,3,4 i.e. 2^2, 2^3, 2^4 downsampling of input
  rpn_anchor_generator:
    _target_: segmentation.models.rpn.anchor_generator.AnchorGenerator
    sizes:
      - [16, 24, 32]
      - [32, 48, 64]
      - [64, 96, 128]
    aspect_ratios:
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
    aspect_ratios_z:
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
  rpn_head: 
    _target_: segmentation.models.rpn.rpn_head.RPNHead
    in_channels: ${backbone_out_channels} 
    num_anchors: 27 # 3 sizes * 3 aspect ratios * 3 aspect ratios_z (TODO: make this dynamic)

datasets:
  batch_size: 1 # total batch size (batch_size per gpu * number of gpus) 
  database:
    _target_: segmentation.data.datasets.skittlez.Skittlez_Database
    training: true # use train or test dataset images
    distributed: false # true # set to true for distributed training (Ray or DDP)
    batch_config:
      _target_: segmentation.data.data_utils.DataConfig
      z: 128 
      y: 256
      x: 256
      c: 3
      color_mode: MATCH  

transforms:
  transforms_list:
    - _target_: segmentation.data.transforms.transforms.Normalize
      # mean: ${dataset_mean}
      # std: ${dataset_std}
    - _target_: segmentation.data.transforms.transforms.Resize
      size: [256, 256, 256]

# ----------------

# Skittlez eval parameters
backbone_out_channels: 256
backbone_target: segmentation.models.backbones.hiera.Hiera
output_features: ["p0", "p1", "p2"] # , "pool" (p^i downsamples starting shape by 2^i+1)

return_intermediates: true # whether to return intermediate features from the backbone

eval_type: skittlez_eval
data_dir: /clusterfs/nvme/segment_4d/test_12
results_dir: /clusterfs/nvme/segment_4d/test_12/evaluation

train_db_savedir: ${data_dir}/db
checkpointdir: /clusterfs/nvme/segment_4d/test_12/checkpoints
ckpt_suffix: "best" # generally, "last" or "best" or "latest"

# evaluator parameters
warmup_iters: 1

# Compute resources
workers: 1 # number of worker nodes
gpu_workers: 1 # number of gpus per worker
cpu_workers: 4 # number of cpu cores per worker (num gpus per worker * number of cpu cores per gpu)
distributed_sampler: false

# Quantization
amp: fp16  # ["no", "fp16", "bf16", "fp8"]

# DeepSpeed config for checkpoint loading
deepspeed_config:
  zero_optimization:
    stage: null # run eval on 1 gpu, no need for zero optimization 