defaults:
  - _self_

name: skittlez

batch_size: 12 # 4 # total batch size (batch_size per gpu * number of gpus) 

dataset_mean: [1000.0] # not currently used, placeholder for dataset mean
dataset_std: [1000.0] # not currently used, placeholder for dataset std

# Data Parameters
channel_in: 3
num_classes: 2 
input_shape: 256
inputs:
  - ${datasets.batch_size}
  - ${datasets.channel_in}
  - ${datasets.input_shape}
  - ${datasets.input_shape}
  - ${datasets.input_shape}

database:
  _target_: segmentation.data.datasets.skittlez.Skittlez_Database
  distributed: true # true # set to true for distributed training (Ray or DDP)
  db_path: /clusterfs/nvme/segment_4d/final_pipeline_v3/db/segmentation_curation_test.db
  train_db_savedir: ${...train_db_savedir} # defer interpolation to main config
  selection_criteria: {"volume" : 8000}  # Minimum volume in pixels to accept instance
  force_create_db: false
  clean_up_db: false
  with_zarr: false
  with_tiff: true
  metadata: null
  batch_config:
    _target_: segmentation.data.data_utils.DataConfig
    z: 128 #128
    y: 128
    x: 128
    c: 3
    color_mode: MATCH  

# MIGRATED TO TRANSFORMS CONFIG
# transforms:
#   - _target_: segmentation.data.transforms.transforms.Normalize
#     # mean: ${dataset_mean}
#     # std: ${dataset_std}
#   - _target_: segmentation.data.transforms.transforms.Resize
#     size: [128, 128, 128] # ensure that z-axis is length L in case z-axis image len is smaller than L

split:  0.1 # null # 0.1 
return_dataloader: true 
workers: 1     
collate_fn: segmentation.data.data_utils.collate_fn_segmentation
worker_init_fn: segmentation.data.data_utils.worker_init_fn_db