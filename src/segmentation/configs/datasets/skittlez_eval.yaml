defaults:
  - _self_

name: skittlez

training: true #false (set to true to check model ability to overfit on train dataset) 
distributed: false # true # set to true for distributed training (Ray or DDP)

batch_size: 4 # 4 # total batch size (batch_size per gpu * number of gpus) 

dataset_mean: [1000.0] # not currently used, placeholder for dataset mean
dataset_std: [1000.0] # not currently used, placeholder for dataset std

# Data Parameters
channel_in: 3
num_classes: 2 
input_shape: 256
inputs:
  - ${datasets.batch_size}
  - ${datasets.channel_in}
  - ${datasets.input_shape}
  - ${datasets.input_shape}
  - ${datasets.input_shape}

database:
  _target_: segmentation.data.datasets.skittlez.Skittlez_Database
  distributed: false # true # set to true for distributed training (Ray or DDP)
  db_path: /clusterfs/nvme/segment_4d/final_pipeline_v3/db/segmentation_curation_test.db
  train_db_savedir: ${...train_db_savedir} # defer interpolation to eval config
  selection_criteria: {"volume" : 8000}  # Minimum volume in pixels to accept instance
  force_create_db: false
  clean_up_db: false
  with_zarr: false
  with_tiff: true
  metadata: null
  batch_config:
    _target_: segmentation.data.data_utils.DataConfig
    z: 128 #128
    y: 128
    x: 128
    c: 3
    color_mode: MATCH  

split:  null # null # 0.1 
return_dataloader: true 
workers: 1     
collate_fn: segmentation.data.data_utils.collate_fn_segmentation
worker_init_fn: segmentation.data.data_utils.worker_init_fn_db