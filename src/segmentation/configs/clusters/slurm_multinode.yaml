defaults:
  - _self_

launcher_type: slurm # type of launcher to use

# (IMPORTANT) NOTE 1: 
# we support two usage patterns for slurm multinode training:
# (1) set number of nodes and necessitate that nodes are exlusive OR
# (2) do not set nodes, instead only set total number of cpus and gpus
#     per task and let Slurm allocate nodes as needed
#     in this case the user must be OK with sharing nodes / spreading 
#     tasks across nodes which may incurr higher communication costs
#     but allow for faster scheduling of jobs in resource-constrained 
#     environments

# NOTE 2: we first determine number of nodes to use based on the number of gpus
#        `next, we allocate Ray worker nodes based on the number of nodes 
#        (1 worker per node). Afterwards, Ray will create Actors in accordance
#        with the total number of gpus per worker. So for example, on a node 
#        with 4 gpus, we will create 4 actors that all live within the same cgroup
#        specified by the original slurm srun allocation.

# Ray cluster configuration
head_node_script: /clusterfs/nvme/hph/git_managed/segmentation/src/segmentation/clusters/ray_slurm_cluster_job.sh
workers_script: /clusterfs/nvme/hph/git_managed/segmentation/src/segmentation/clusters/ray_slurm_workers.sh
script: /clusterfs/nvme/hph/git_managed/segmentation/src/segmentation/train/train_segmentation.py
python_env: /usr/bin/python # python executable to use (if not using apptainer)                     
job_name: ${network}_segmentation_multinode      # name of the job      
mount_path: /clusterfs/nvme/segment_4d  # path to mount for the job
src: /clusterfs/nvme/hph/git_managed/segmentation/src/segmentation # path to the repo directory

task_names: []         # any additional arguments to pass to train run script
tasks: []         # arg values to pass
launcher_logdir: ${logdir}/launcher_logs     # directory to store launcher/worker logs & outputs

partition: abc_a100   # partition to submit the job to
qos: abc_high   # qos for the job

batch_size: 12 # total batch size = (batch_size per gpu * number of gpus)

total_gpus: 12          # total number of gpus to use
gpus_per_worker: 4          # number of gpus to use per node
mem_per_worker: 31000      # memory per node
cpus_per_worker: 4          # number of cpus to use per node  
workers: 3          # number of nodes to use 

exclusive: true      # whether to use exclusive node allocation
nodelist: null       # list of specific nodes to use
dependency: null       # job dependency
constraint: null       # constraint for the job
timelimit: null       # time limit for the job 
max_worker_heartbeat_timeout: 2 # in minutes (to prevent indefinite hang if one process fails)

qos_head_node: abc_high # qos for the head node
partition_head_node: abc # partition for the head node
total_gpus_head_node: 0 # total number of gpus to use for the head node
workers_head_node: 1 # number of workers to use for the head node
cpus_per_task_head_node: 24 # number of cpus to use for the head node
mem_per_cpu_head_node: 31000 # memory to use for the head node

# apptainer image to use for containerized workerloads
apptainer: /clusterfs/nvme/hph/git_managed/env_images/develop_torch_cuda_12_8_ops3d_v2.sif

# script to launch ray slurm cluster 
ray: /clusterfs/nvme/hph/git_managed/train/segmentation/src/segmentation/clusters

# Ray ScalingConfig (some variables are set to null to be set in the training loop)
scaling_config:
  num_workers: null # int(cfg.clusters.workers) * int(cfg.clusters.gpus_per_worker)
  use_gpu: true
  resources_per_worker:
    CPU: null # int(cfg.cpus_per_worker) // int(cfg.gpus_per_worker)
    GPU: 1
  trainer_resources:
    CPU: 0

# Ray RunConfig
run_config:
  log_to_file: true
  storage_path: ${outdir}
  checkpoint_config:
    num_to_keep: 3
    checkpoint_score_attribute: loss
    checkpoint_score_order: min
  failure_config:
    max_failures: 0
  
# torch Config
torch_config:
  timeout_s: 3600