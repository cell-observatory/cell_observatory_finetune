defaults:
  - _self_

seed: 42 # random seed for reproducibility

# deepSpeed config (Hydra will pass it to training loop)
fp16:
  enabled: true
  auto_cast: true
  loss_scale: 0 #  dynamic loss scaling (0 = auto)
  loss_scale_window: 1000 # number of good steps before increasing the scale again.
  hysteresis: 2 # number of overflows to wait before decreasing the loss scale.
  consecutive_hysteresis: true # if true, hysteresis must be consecutive (i.e., N overflows in a row)
  min_loss_scale: 1 # minimum loss scale to use
bf16:
  enabled: false 
  auto_cast: false
zero_optimization:
  stage: 0 # full ZeRO-3 with parameter + gradient + optimizer partitioning
  reduce_bucket_size: auto # controls how many elements are grouped before being reduced
  reduce_scatter: true # fused reduce + scatter ops
  contiguous_gradients: true # gradients allocated in a contiguous memory block
  overlap_comm: true # overlap communication with computation
  stage3_prefetch_bucket_size: auto # params to prefetch in advance
  stage3_param_persistence_threshold: auto # keep parameters in memory if theyâ€™re small and reused often
  stage3_max_live_parameters: 1e9 
  stage3_max_reuse_distance: 1e9
  stage3_gather_16bit_weights_on_model_save: true # reconstruct full weights (in FP16) during saving, rather than saving sharded pieces
  offload_optimizer: # disables offloading to CPU, everything stays on GPU
    device: none 
  offload_param:
    device: none
tensorboard:
  enabled: true
  output_path: ${logdir}
  job_name: ${outdir}
csv_monitor:
  enabled: true
  output_path: ${logdir}
  job_name: ${outdir}

gradient_clipping: ${optimizers.clip_grad}

steps_per_print: 100

gradient_accumulation_steps: 1 # number of steps to accumulate gradients before performing a backward/update pass

train_batch_size: ${clusters.batch_size}

zero_allow_untested_optimizer: true

profile: false
flops_profiler:
  enabled: true
  profile_step: 1
  module_depth: -1
  top_modules: 1
  detailed: true
  output_file: ${logdir}/flops_profiler.log