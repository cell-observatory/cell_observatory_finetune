defaults:
  - _self_

# FPN 

_target_: segmentation.models.backbones.fpn.BackboneWithFPN
# NOTE: FPN may add an extra LastLevelMaxPool layer to the last feature map
# Hence, the output feature maps may have len(return_layers) + 1 maps
# return layers specifies which feature maps to run through the FPN
return_layers: {"p0": "p0", "p1": "p1"} # "p2": "p2"
in_channels_list: [96, 192] # 384, 768 (matches embed_dim * stage sequence)
out_channels: ${backbone_out_channels}
norm_layer: null

# backbone

backbone:
  _target_: ${backbone_target}

  return_intermediates: true # whether to return intermediate features from the backbone

  # TODO: make input_size dynamic based on the dataset
  input_size: [256, 256, 256]
  in_chans: ${datasets.channel_in}
  num_classes: ${datasets.num_classes}
  embed_dim: 96  # initial embed dim
  num_heads: 1  # initial number of heads

  # patch_embed -> stage 0 => 64x64x64, stage 1 => 32x32x32, stage 2 => 16x16x16 due to q-pooling 
  stages: [2, 3, 16] # , 3
  q_pool: 2 # 3 number of q_pool stages
  q_stride: [2, 2, 2]
  mask_unit_size: [8, 8, 8]  # must divide q_stride ** (#stages-1)
  # mask_unit_attn: which stages use mask unit attention?
  mask_unit_attn: [True, True, False, False]

  dim_mul: 2.0 # 4 stages =>  embed_dim = (96, 192, 384, 768)
  head_mul: 2.0 # 4 stages => num_heads = (1, 2, 4, 8)

  patch_kernel: [7, 7, 7] # for conv patch_embed, 4x downsample
  patch_stride: [4, 4, 4] 
  patch_padding: [3, 3, 3]

  mlp_ratio: 4.0
  drop_path_rate: 0.0
  norm_layer: "LN"
  head_dropout: 0.0
  head_init_scale: 0.001
  sep_pos_embed: False