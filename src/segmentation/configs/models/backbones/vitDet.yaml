defaults:
  - _self_

# VitDet INFO (2D Standard): 
# Input: [B, 3, 1024, 1024] → Patch embedding: [B, C, 64, 64] -> [B, 4096, C] if global attention
#        OR [num_ws, (ws×ws), C] for window attention
#        ws = 14x14 commonly => operates on [num_ws, C, 14, 14] local window grid (roughly 4x4 windows per image)

# Simple Feature Pyramid

_target_: ${backbone_target}
in_feature: "last_feat"
out_channels: ${backbone_out_channels} # 256 generally
scale_factors: [4.0, 2.0, 1.0] # , 0.5 (scale factors dictate output feature map size, here: p4 -> p3 -> p2)
top_block: null
# top_block: 
#   _target_: segmentation.models.backbones.vitDet.LastLevelMaxPool # if add last level max pool to PN
norm: "LN"
square_pad: null # not used currently

# ViTDet Backbone (adapted patch sizes, window stages etc. to make sense for 3D data)

net:
  _target_: segmentation.models.backbones.vitDet.ViT
  img_size: ${datasets.input_shape} 
  channel_in: ${datasets.channel_in} 
  weights: null
  # Rationale from VitDet Paper: with patch size 16×16 and 3 colors, 
  # a hidden dimension ≥768 (ViT-B and larger) can preserve all information
  # of a patch if necessary
  # Note: ViT-H by default patch size of 14 => interpolate patch embedding
  #                                         => 14×14×3 to 16×16×3
  patch_size: 16 # sets patch_size and stride in PatchEmbed 3D Conv (usually 16)
  embed_dim: 768 # 1280 for H, 1024 for L, 768 for B
  depth: 12 # 32 for H, 24 for L, 12 for B
  num_heads: 12 # 16 for H, 16 for L, 12 for B
  mlp_ratio: 4 
  qkv_bias: true
  drop_path_rate: 0.1 # 0.4 for H, 0.5 for H, 0.1 for B
  norm_layer: "LN"
  # act_layer: "GELU" # TODO: implement get_layer functionality as with get_norm
  use_abs_pos: false # true in VitDet with pretrained weights  
  use_rel_pos: true # false 
  rel_pos_zero_init: True 
  window_size: 8 # commonly 14x14
  # window attention not currently used so VitDet currently functions as a vanilla Vit with a simple pyramid network added
  # i.e. global attention only on 256/16 = 16 x 16 x 16 patches upsampled to 32^3 and 64^3 from last feature map
  # H: list(range(0, 7)) + list(range(8, 15)) + list(range(16, 23)) + list(range(24, 31)), 5, 11, 17, 23 for global attention 
  # L: list(range(0, 5)) + list(range(6, 11)) + list(range(12, 17)) + list(range(18, 23)), 7, 15, 23, 31 for global attention 
  # B: [0,1,3,4,6,7,9,10], 2, 5, 8 11 for global attention
  window_block_indexes: [] # fully blocked for B: [0,1,2,3,4,5,6,7,8,9,10]   
  residual_block_indexes: [] # not used currently
  pretrain_img_size: 224 # not used currently
  pretrain_use_cls_token: True # not used currently
  out_feature: "last_feat"
