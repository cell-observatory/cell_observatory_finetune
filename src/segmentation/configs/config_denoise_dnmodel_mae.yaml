defaults:
  - models: encoder_decoder # Loads configs/models/encoder_decoder.yaml
  - models/preprocessors: preprocessor # Loads configs/models/preprocessors/preprocessor.yaml
  - models/backbones: masked_encoder_wrapper # Loads configs/models/backbones/masked_encoder_wrapper.yaml
  - models/heads: masked_predictor_wrapper # Loads configs/models/heads/masked_predictor_wrapper.yaml
  - models/mask_generators: mask_generator # Loads configs/models/mask_generators/mask_generator.yaml
  - models/patch_embeddings: patch_embedding # Loads configs/models/patch_embeddings/patch_embedding.yaml
  - losses: mae_loss # Loads configs/losses/mae_loss.yaml
  - datasets: denoise # Loads configs/datasets/denoise.yaml
  - transforms: transforms_denoise # Loads configs/transforms/transforms_denoise.yaml
  - optimizers: adamw # Loads configs/optimizers/adamw.yaml
  - schedulers: warmup_cosine_decay # Loads configs/schedulers/warmup_cosine_decay.yaml
  - metrics: metrics # Loads configs/metrics/metrics.yaml
  - deepspeed: deepspeed # Loads configs/deepspeed/deepspeed.yaml
  - clusters: local # Loads configs/clusters/slurm_multinode.yaml
  - _self_  # self reference last to allow for overrides

# overrides
# ----------------

clusters:
  mount_path: /clusterfs/vast/matthewmueller/dataTest/2025
  batch_size:               1          # NOTE: batch size should divide im2col_step in deformable attn
  total_cpus:               4          # total number of cpus to use
  total_gpus:               1          # total number of gpus to use
  gpus_per_worker:          1          # number of gpus to use per node
  mem_per_worker:           31000      # memory per node
  cpus_per_worker:          4          # number of cpus to use per node  
  workers:                  1          # number of nodes to use 

models:
  masking: false # enable masking
  backbones:
    channel_predict: false
  heads:
    output_embed_dim: 8192
    channel_predict: false
  losses:
    patchify: false
  patch_embeddings:
    input_shape:
      - 1    # B
      - 1    # T
      - 128  # Z
      - 128  # Y
      - 128  # X 
      - 2  # C
    channels: 2

datasets:
  input_format: BTZYXC
  input_shape: 
      - 1 # B
      - 1 # T
      - 128 # Z
      - 128 # Y
      - 128 # X 
      - 2 # C
  channel_in: 2
  num_classes: 2
  databases:
    tile: true
    with_labels: true

    db_readpath: /clusterfs/nvme/segment_4d/testing2/db_tables_test9
    db_savepath: /clusterfs/nvme/segment_4d/testing2/db_tables_test9

    db_read_method: "feather"
    db_save_method: "feather"
    force_create_db: false

    # FIXME: here TCZYX but elsewhere TZYXC
    data_tile: [1, 2, 128, 128, 128]
    label_tile: [1, 128, 128, 128]

    label_spec:
      _target_: segmentation.data.databases.schema.DB_LabelSchema
      # table name based on database nomenclature
      table: "prepared"
      # since we don't have denoise
      # metadata <-> data right now, we use the same table
      # as for data to test pipeline, will probably change in the future
      label_metadata:
        id: null
        server_folder: null
        output_folder: null
        z_start: null
        z_end: null
        y_start: null
        y_end: null
        x_start: null
        x_end: null

# ----------------

# Model type
network: mae

# Training paradigm
paradigm: segmentation.train.backend_segmentation.supervised

# Paths
outdir: /clusterfs/nvme/segment_4d/testing_2
logdir: ${outdir}/logs

load_checkpointdir: null # specify if we want to load a checkpoint from a previous run that is not latest local run
checkpointdir: ${outdir}/checkpoints # specify where to save the checkpoints
checkpoint_update_interval: 100 # save checkpoint every N epochs

val_interval: 1
log_step: 100 # step interval to show training loss in train loop
val_log_step: 20 # step interval to show loss in val loop