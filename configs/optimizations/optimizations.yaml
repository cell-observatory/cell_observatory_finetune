defaults:
  - _self_

cudnn_benchmark: true
cudnn_enabled: true
cudnn_allow_tf32: true
cudnn_deterministic: false
deterministic: false

activation_checkpoint:
  enabled: false
  # modules to activation checkpoint (either module with list of transformer blocks or
  # a list of transformer blocks)
  modules: ["masked_encoder.encoder", "masked_decoder.encoder.transformer_blocks"]
  blocks_nomenclature: "transformer_blocks"
  # options: "op" or a positive int representing layer frequency
  selective_ac_option: null
  # list of fully qualified names (FQNs) of modules to force recompute 
  # of any modules w. same shapes, applied when selective_ac_option=="op"
  # uses selective activation checkpointing per operation
  # this op should match "layers.layer_id.module_fqn" where layer_id is the layer index
  # you get from model.layers.named_children() and where module_fqn is the name 
  # you get from module.named_modules()
  per_op_sac_force_recompute_mm_shapes_by_fqns: null
  # `full` applies activation checkpointing to all layers whereas `selective`
  # either applies it to layers with a specific frequency or to specific modules, see
  # per_op_sac_force_recompute_mm_shapes_by_fqns
  mode: "selective"  # options: "full", "selective"
  mm_recompute_frac: 2

torch_compile:
  enabled: false
  # modules to compile
  modules: ["masked_encoder.encoder", "masked_decoder.encoder.transformer_blocks"]
  blocks_nomenclature: "transformer_blocks"
  range: block_based # options: "full", "block_based"
  mode: reduce-overhead
  dynamic: false
  blockbased_fullgraph: false

env:
  HYDRA_FULL_ERROR: true            # print full Hydra tracebacks
  RAY_DEDUP_LOGS: true
  TORCH_DISTRIBUTED_DEBUG: null     # e.g. "INFO"
  NCCL_DEBUG_SUBSYS: null           # e.g. "GRAPH"
  OMP_NUM_THREADS: null             # e.g. "1"
  NCCL_CUMEM_ENABLE: null           # e.g. "0"
  NCCL_CROSS_NIC: true
  NCCL_P2P_LEVEL: "NVL"
  TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 3600
  PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
  NVIDIA_TF32_OVERRIDE: "1"

with_model_summary: false