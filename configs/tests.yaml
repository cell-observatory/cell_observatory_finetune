defaults:
  - clusters: local                          # configs/clusters/local.yaml
  - paths: local                             # configs/paths/local.yaml
  - datasets: pretrain_dataset_ray           # configs/datasets/pretrain_dataset_ray.yaml
  - models: mae/tiny                         # configs/models/mae/tiny.yaml
  - hooks: hooks_with_weight_decay           # configs/hooks/hooks_with_weight_decay.yaml
  - checkpoint: checkpoint                   # configs/checkpoint/checkpoint.yaml
  - loggers: loggers                         # configs/loggers/loggers.yaml
  - evaluation: base_evaluator               # configs/evaluation/base_evaluator.yaml
  - deepspeed: zero3                         # configs/deepspeed/zero3.yaml
  - optimizers: lamb                         # configs/optimizers/lamb.yaml
  - schedulers: test_warmup_cosine_decay     # configs/schedulers/test_warmup_cosine_decay.yaml
  - optimizations: optimizations             # configs/optimizations/optimizations.yaml
  - profiling: profiling                     # configs/profiling/profiling.yaml
  - _self_


# experiment name (used for logging)
experiment_name: tests
wandb_project: platform_github_action_tests

# task
task: pretrain_mae

# Model type
network: mae

# engine type
engine: deepspeed

# distributed framework
distributed_framework: ray

# run type
run_type: single_run # single_run, multi_run, or tune

# job type
job_type: train # train or test

# trainer type
trainer: training.loops.EpochBasedTrainer

# benchmark loop to run for each worker
loop_per_worker_script: training.loops.train_loop_per_worker

# training quantization
quantization: bfloat16 # data type for training, options: bfloat16, float32, float16
storage_dtype: uint16 # data type for storage

# dtype for training data
# NOTE: numpy doesn't support bfloat16, the preprocessor will cast data before we feed it to the model 
dataset_dtype: float16 # data type for loading data, options: float32, float16 


# dataset layout in the zarr files
dataset_layout_order: TZYXC

# overrides
# --------------------------------------------------------------------------------

paths:
  # base output directory for logs, checkpoints, etc.
  outdir: ${paths.data_path}/pretrained_models/${experiment_name}
  resume_checkpointdir: null 
  pretrained_checkpointdir: null

clusters:
  batch_size: 2 # total batch size
  worker_nodes: 1 # number of worker nodes
  gpus_per_worker: 1 # number of gpus per worker node
  cpus_per_gpu: 8 # number of cpu cores per gpu
  mem_per_cpu: 16000 # ram per cpu core

datasets:
  # mainly used by other configs but determined here for ease of access
  batch_size: ${clusters.batch_size_per_gpu} # batch size = (total_batch_size * total_gpus)
  input_shape: [16, 128, 128, 128, 2] # NOTE: important that this matches database hypercube shape
  patch_shape: [4, 16, 16, 16]

  split: 0.1
  return_dataloader: true
  distributed_sampler: true

  prefetch_factor: 1 # prefetch factor for the dataloader
  num_workers: 4

  max_hypercubes: 100 # maximum number of hypercubes to return
  use_cached_hypercubes_dataframe: true
  # hypercubes_dataframe_path: ${paths.server_folder_path}/databases/hypercubes_dataframe.csv
  hypercubes_dataframe_path: /clusterfs/nvme/segment_4d/pretrained_models/benchmark_dataloader_with_obs_new/database/hypercubes_dataframe.csv
  server_folder_path: /clusterfs/vast/Data/read_speed_tests/large_test_zarrs

  exec_async: true
  exec_pipelined: true

evaluation:
  val_begin: 0
  val_interval: 1
  # used by BestMetricSaver to save best metric
  # needs to match one of metrics saved per epoch by loss_dict
  # in the training loop or evaluator.evaluate() in the evaluation loop
  val_metric: val_step_loss
  val_mode: min # mode for the validation metric, can be 'max' or 'min'
  evaluator:
    _target_: evaluation.base_evaluation.BaseEvaluator
    training_metrics:
      - "step_loss": "mean"

loggers:
  event_writers:
      - _target_: cell_observatory_platform.training.loggers.LocalEventWriter
        save_dir: ${loggers.logdir}
        step_scalars_prefix: step_logbook
        epoch_scalars_prefix: epoch_logbook
        scalars_save_format: csv

hooks:
  hooks_list:
  - _target_: cell_observatory_platform.training.hooks.WeightDecayScheduleHook
  - _target_: cell_observatory_platform.training.hooks.AnomalyDetector
  - _target_: cell_observatory_platform.training.hooks.SamplerSetter
  - _target_: cell_observatory_platform.training.hooks.LRScheduler
  - _target_: cell_observatory_platform.training.hooks.WeightDecayScheduleHook
  - _target_: cell_observatory_platform.training.hooks.IterationTimer
    warmup_iter: 2
  - _target_: cell_observatory_platform.training.hooks.PeriodicWriter
  - _target_: cell_observatory_platform.training.hooks.PeriodicCheckpointer
    period: 1
    file_prefix: latest_model
  - _target_: cell_observatory_platform.training.hooks.BestCheckpointer
    checkpointdir: ${checkpoint.checkpoint_manager.save_checkpointdir}
  - _target_: cell_observatory_platform.training.hooks.TorchProfiler
    output_dir: ${loggers.logdir}/profiler
    schedule:
      skip_first: 1
      warmup: 1
      active: 3
      repeat: 2
      wait: 1
    activities:
      - CUDA
      - CPU
    save_tensorboard: true
    shutdown_after_profile: False
  - _target_: cell_observatory_platform.training.hooks.TorchMemoryStats
    step_period: 5
    epoch_period: 1
    logdir: ${loggers.logdir}
  - _target_: cell_observatory_platform.training.hooks.BestMetricSaver
    metric_name: ${evaluation.val_metric}
    compare_fn: ${evaluation.val_mode}
    eval_after_validation: true
  - _target_: cell_observatory_platform.training.hooks.EarlyStopHook
    patience: 10
    stopping_threshold: 0.01
    mode: ${evaluation.val_mode}
    metric_name: ${evaluation.val_metric}