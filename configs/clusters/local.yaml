defaults:
  - _self_

launcher_type: local # type of launcher to use

framework: ray # framework to use for distributed training

# Ray cluster configuration
script: /clusterfs/nvme/hph/git_managed/cell_observatory_finetune/train/train_segmentation.py
# apptainer image to use for containerized workerloads
apptainer: /clusterfs/nvme/hph/git_managed/env_images/develop_torch_cuda_12_8_ops3d.sif
python: python # python executable to use (if not using apptainer)                     
job_name: ${network}_segmentation_local      # name of the job      

task_names: [] # any additional arguments to pass to train run script
tasks: [] # arg values to pass
launcher_logdir: ${logging.logdir}/launcher_logs     # directory to store launcher/worker logs & outputs

partition: abc_a100   # partition to submit the job to
qos: abc_high   # qos for the job

batch_size: 1 # total batch size = (batch_size per gpu * number of gpus)
worker_batch_size: 1

total_cpus: 4          # total number of cpus to use
total_gpus: 1          # total number of gpus to use
gpus_per_worker: 1          # number of gpus to use per node
mem_per_worker: 31000      # memory per node
cpus_per_worker: 4          # number of cpus to use per node  
workers: 1          # number of nodes to use 

exclusive: true      # whether to use exclusive node allocation
nodelist: null       # list of specific nodes to use
dependency: null       # job dependency
constraint: null       # constraint for the job
timelimit: null       # time limit for the job
max_worker_heartbeat_timeout: 2 # in minutes (to prevent indefinite hang if one process fails)

# Ray ScalingConfig (some variables are set to null to be set in the training loop)
scaling_config:
  num_workers: ${clusters.total_gpus} # int(cfg.clusters.workers) * int(cfg.clusters.gpus_per_worker)
  use_gpu: true
  resources_per_worker:
    CPU: ${clusters.cpus_per_worker} # int(cfg.cpus_per_worker) // int(cfg.gpus_per_worker)
    GPU: 1
  trainer_resources:
    CPU: 0
    
# Ray RunConfig
run_config:
  log_to_file: true
  storage_path: ${outdir}
  failure_config:
    max_failures: 0
  
# torch Config
torch_config:
  timeout_s: 3600