defaults:
  - _self_

# NOTE: some configs here should be moved to own .yaml!!!

_target_: finetune.models.heads.masked_predictor_wrapper.MaskedPredictorWrapper

channel_predict: false # perform channel predict task
num_channels: ${datasets.channel_in} # number of channels in input data

# template options:
# 'mp-tiny',
# 'mp-small',
# 'mp-base',
# 'mp-large',
# 'mp-huge',
# 'mp-giant',
# 'mp-gigantic'

model_template: mp # custom use `embed_dim`, `depth`, `num_heads` and `mlp_ratio` to config model
# FIX LAYOUT IN PATCH EMBEDDINGS AND DATA STRUCTURES
input_fmt: BTZYXC
input_shape: 
  - 1    # B
  - 1    # T
  - 128  # Z
  - 128  # Y
  - 128  # X 
  - ${datasets.channel_in} # C
lateral_patch_size: ${models.backbones.lateral_patch_size}
axial_patch_size: ${models.backbones.axial_patch_size}
temporal_patch_size: ${models.backbones.temporal_patch_size}
input_embed_dim: 768
# for 3D = (lateral_patch_size * axial_patch_size * axial_patch_size * out_channels)
# example: for 128x128x128 input shape and 16x16x16 patch size, it will be 16x16x16x2 = 8192
output_embed_dim: 8192
embed_dim: 384
depth: 12
num_heads: 12
mlp_ratio: 4.0
proj_drop_rate: 0.0
att_drop_rate: 0.0
drop_path_rate: 0.1
init_std: 0.02
fixed_dropout_depth: False
# norm layers: 'RmsNorm', 'LayerNorm', 'SyncBatchNorm', 'GroupNorm' 
norm_layer: RmsNorm
# act layers: 'GELU', 'SiLU', 'LeakyReLU', 'GLU', 'Sigmoid', 'Tanh'
act_layer: SiLU
# mlp layers: 'Mlp', 'SwiGLU'
mlp_layer: SwiGLU