defaults:
  - _self_

# NOTE: not used change how masked encoder wrapper and patch_embeddings are initialized!!!

_target_: train_platform.models.patch_embeddings.PatchEmbedding

# input_fmt: ${datasets.input_format}
# TODO: unify layout between data structures and patch_embeddings
input_fmt: BTZYXC
input_shape:
  - 1    # B
  - 1    # T
  - 128  # Z
  - 128  # Y
  - 128  # X 
  - 2
lateral_patch_size: ${models.backbones.lateral_patch_size}
axial_patch_size: ${models.backbones.axial_patch_size}
temporal_patch_size: ${models.backbones.temporal_patch_size}
# TODO: masked encoder currently does not support temporal patch size
# temporal_patch_size: ${models.backbones.masked_encoder_wrapper.temporal_patch_size}
embed_dim: ${models.backbones.embed_dim}
channels: 2