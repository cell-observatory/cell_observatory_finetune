_target_: cell_observatory_finetune.models.meta_arch.maskdino.MaskDINO

# resources: 
# https://arxiv.org/pdf/2206.02777
# https://github.com/facebookresearch/dinov3/dinov3/eval/segmentation/models/backbone/dinov3_adapter.py

# backbone (MAE/JEPA/ETC.)
backbone:
  _target_: cell_observatory_finetune.models.meta_arch.maskedautoencoder.FinetuneMaskedAutoEncoder
  input_fmt: ${dataset_layout_order}
  input_shape: ${datasets.input_shape}
  patch_shape: ${datasets.patch_shape}

  embed_dim: 1024
  depth: 24
  num_heads: 16
  mlp_ratio: 4.0

  proj_drop_rate: 0.0
  att_drop_rate: 0.0
  drop_path_rate: 0.1
  init_std: 0.02
  fixed_dropout_depth: false
  norm_layer: RmsNorm
  act_layer: SiLU
  mlp_layer: SwiGLU

  abs_sincos_enc: false
  rope_pos_enc: true
  rope_random_rotation_per_head: true
  rope_mixed: false
  rope_theta: 100.0
  weight_init_type: mae
  mlp_wide_silu: false
  loss_fn: l2_masked

  decoder_args: null
  decoder: "maskdino"
  task: "instance_segmentation"
  output_channels: null

# NOTE: they seem to use defaults in DINOv3 for adapter when used with plainDETR

# backbone adapter
input_format: ${dataset_layout_order}
input_shape: ${datasets.input_shape}
patch_shape: ${datasets.patch_shape}
with_backbone_adapter: true
dim: 3
backbone_embed_dim: ${models.backbone.embed_dim}
num_backbone_features: 4
add_vit_feature: true
conv_inplane: 64
use_deform_attention: true 
n_points: 4
deform_num_heads: 16
drop_path_rate: 0.3
init_values: 0.0
with_cffn: true
cffn_ratio: 0.5
deform_ratio: 0.5
use_extra_extractor: true
strategy: axial
spatial_prior_module_strides:
  stem1: [2,2,2]
  stem2: [2,2,2]
  stem3: [1,1,1]
  maxpool: 2
  stage2: [2,2,2]
  stage3: [2,2,2]
  stage4: [2,2,2]

# MaskDINOEncoder (pixel decoder)
input_shape_metadata: 
  "1": 
    channels:  ${models.backbone.embed_dim}
    stride: 4
  "2": 
    channels: ${models.backbone.embed_dim}
    stride: 8
  "3": 
    channels: ${models.backbone.embed_dim}
    stride: 16
  "4":
    channels: ${models.backbone.embed_dim}
    stride: 32

# potentially use 2,3,4 and keep total_num_feature_levels=4
transformer_in_features: ["1", "2", "3", "4"] 
target_min_stride: null
total_num_feature_levels: 4
transformer_encoder_dropout: 0.1
transformer_encoder_num_heads: 8
transformer_encoder_dim_feedforward: 1024
num_transformer_encoder_layers: 4
conv_dim: 288 # NOTE: needs to be multiple of 32 for GroupNorm and 3 for pos encodings!
mask_dim: 256
norm: null

# MaskDINODecoder
in_channels: ${models.backbone.embed_dim}
num_classes: 1
hidden_dim: 256
num_queries: 100
feedforward_dim: 2048
decoder_num_layers: 4
enforce_input_projection: true
two_stage_flag: true
denoise_queries_flag: true
noise_scale: 0.4
total_denosing_queries: 100
initialize_box_type: bitmask
with_initial_prediction: true
learn_query_embeddings: false
dropout: 0.1
activation: RELU
num_heads: 8
decoder_num_points: 4
return_intermediates_decoder: true
query_dim: 6
share_decoder_layers: false

# matchers

# NOTE: COST=LOSS WEIGHTS

cost_classification: 4.0 
cost_mask: 5.0
cost_mask_dice: 5.0
num_points: 1404928 # 112*112*112 (same as original implementation in 3D)
cost_box: 5.0
cost_box_giou: 2.0

# "Therefore, the total loss is a linear combination of three kinds of losses:
# λ_clsL_cls + λ_L1L_L1 + λ_giouL_giou + λ_ceL_ce + λ_diceL_dice,
# where we set λ_cls = 4, λ_L1 = 5, λ_giou = 2, λ_ce = 5, and
# λ_dice = 5."

# criterion
no_object_loss_weight: 0.1
losses: ["loss_mask", "loss_dice", "loss_ce", "loss_bbox", "loss_giou"]
oversample_ratio: 3.0
importance_sample_ratio: 0.75
denoise: true
with_segmentation: true
denoise_losses: ["loss_mask", "loss_dice", "loss_ce", "loss_bbox", "loss_giou"]
semantic_ce_loss: false
focal_alpha: 0.25
loss_weight_dict:
  loss_ce: 4.0
  loss_bbox: 5.0
  loss_giou: 2.0
  loss_mask: 5.0
  loss_dice: 5.0

# training parameters
instance_segmentation_flag: true
topk_per_image: 100
focus_on_boxes: false