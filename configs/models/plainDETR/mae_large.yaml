_target_: cell_observatory_finetune.models.meta_arch.plainDETR.PlainDETRReParam

# resources: 
# https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_DETR_Does_Not_Need_Multi-Scale_or_Locality_Design_ICCV_2023_paper.pdf
# https://github.com/facebookresearch/dinov3/dinov3/eval/segmentation/models/backbone/dinov3_adapter.py

# backbone (MAE/JEPA/ETC.)
backbone_wrapper_args:
  backbone_embed_dims: [1024, 1024, 1024, 1024]
  train_backbone: true
  use_layernorm: true
  blocks_to_train: null
  backbone_args:
    model: FinetuneMaskedAutoEncoder
    input_fmt: ${dataset_layout_order}
    input_shape: ${datasets.input_shape}
    patch_shape: ${datasets.patch_shape}
    input_channels: ${tasks.input_channels}

    embed_dim: 1024
    depth: 24
    num_heads: 16
    mlp_ratio: 4.0

    proj_drop_rate: 0.0
    att_drop_rate: 0.0
    drop_path_rate: 0.1
    init_std: 0.02
    fixed_dropout_depth: false
    norm_layer: RmsNorm
    act_layer: SiLU
    mlp_layer: SwiGLU

    abs_sincos_enc: false
    rope_pos_enc: true
    rope_random_rotation_per_head: true
    rope_mixed: false
    rope_theta: 100.0
    weight_init_type: mae
    mlp_wide_silu: false
    loss_fn: l2_masked

    decoder: "plainDETR"
    task: "instance_segmentation"
    output_channels: null
    decoder_args:
      encoder_out_layers: [10, 14, 18, 22]  # use these layers from encoder

# NOTE: they seem to use defaults in DINOv3 for adapter when used with plainDETR

# backbone adapter
adapter_args:
  input_format: ${dataset_layout_order}
  input_shape: ${datasets.input_shape}
  patch_shape: ${datasets.patch_shape}
  input_channels: ${tasks.input_channels}
  dtype: ${quantization}
  dim: 3
  backbone_embed_dim: ${models.backbone_wrapper_args.backbone_args.embed_dim}
  num_backbone_features: 4
  add_vit_feature: true
  conv_inplane: 64
  use_deform_attention: false 
  n_points: 4
  n_levels: 1
  deform_num_heads: 16
  drop_path_rate: 0.3
  init_values: 0.0
  with_cffn: true
  cffn_ratio: 0.5
  deform_ratio: 0.5
  use_extra_extractor: true
  strategy: axial
  spatial_prior_module_strides:
    stem1: [2,2,2]
    stem2: [2,2,2]
    stem3: [1,1,1]
    maxpool: 2
    stage2: [2,2,2]
    stage3: [2,2,2]
    stage4: [2,2,2]

# plainDETR transformer
transformer_args:
  d_model: 384 # d_model divisible by 3 (and ideally by 6)
  nheads: 8
  num_feature_levels: 4
  two_stage: true
  two_stage_num_proposals: 100
  norm_type: "pre_norm"
  decoder_type: "global_rpe_decomp"
  proposal_feature_levels: 4
  proposal_in_stride: 16
  proposal_tgt_strides: [8, 16, 32, 64]
  proposal_min_size: 50
  # transformer_encoder
  add_transformer_encoder: true
  dim_feedforward: 2048
  dropout: 0.0
  activation: "relu"
  normalize_before: true
  num_encoder_layers: 6
  # global decoder
  global_decoder_args:
    hidden_dim: 512
    dropout: 0.0
    proposal_in_stride: 16
    norm_type: "pre_norm"
    dim_feedforward: 2048
    num_heads: 8
    qkv_bias: true
    qk_scale: null
    attn_drop: 0.0
    proj_drop: 0.0
    dec_layers: 6
    look_forward_twice: true
    rpe_hidden_dim: 512
    rpe_type: "linear"
    feature_stride: 16
    reparam: true

# criterion
criterion_args:
  num_classes: 2  # including background
  weight_dict:
    loss_ce: 2.0
    loss_bbox: 5.0
    loss_giou: 2.0
  losses: ["labels", "boxes", "cardinality"]
  focal_alpha: 0.25
  reparam: true
  matcher_args:
    # matchers
    cost_class: 2.0
    cost_bbox: 5.0
    cost_giou: 2.0
    cost_bbox_type: "reparam"

# plainDETR
backbone_embed_dims: [1024, 1024, 1024, 1024]
num_classes: 2
num_feature_levels: 4
aux_loss: true
with_box_refine: true
two_stage: true
num_queries_one2one: 300
num_queries_one2many: 1500
mixed_selection: true
reparam: true
k_one2many: 5
lambda_one2many: 1.0
normalize_pos_encodings: true