# @package _global_
defaults:
  - /tasks/instance_segmentation             # configs/tasks/instance_segmentation.yaml
  - /paths/janelia                           # configs/paths/coreweave.yaml
  - /hydra/overrides                         # configs/hydra/overrides.yaml (not working)
  - /clusters/local                          # configs/clusters/coreweave_h100.yaml
  - /datasets/pretrain_dataset_ray           # configs/datasets/pretrain_dataset.yaml
  - /datasets/preprocessor/instance_segmentation_preprocessor # configs/datasets/preprocessor/instance_segmentation_preprocessor.yaml
  - /datasets/masking/mask_generator         # configs/datasets/masking/mask_generator.yaml
  - /models/plainDETR/mae_large              # configs/models/plainDETR/large.yaml
  - /hooks/hooks_with_weight_decay           # configs/hooks/hooks.yaml
  - /checkpoint/checkpoint                   # configs/checkpoint/checkpoint.yaml
  - /loggers/loggers                         # configs/loggers/loggers.yaml
  - /evaluation/base_evaluator               # configs/evaluation/base_evaluator.yaml
  - /deepspeed/zero2                         # configs/deepspeed/zero2.yaml
  - /optimizers/lamb                     # configs/optimizers/lamb.yaml
  - /schedulers/warmup_cosine_decay_with_weight_decay  # configs/schedulers/test_warmup_cosine_decay.yaml
  - /optimizations/optimizations             # configs/optimizations/optimizations.yaml
  - /profiling/profiling                     # configs/profiling/profiling.yaml
  - _self_

# experiment name (used for logging)
experiment_name: test_instance_segmentation_plain_detr
wandb_project: testing

# task
task: finetune_mae

# Model type
network: plainDETR

# engine type
engine: deepspeed

# distributed framework
distributed_framework: ray

# run type
run_type: single_run # single_run, multi_run, or tune

# job type
job_type: train # train or test

# trainer type
trainer: training.loops.EpochBasedTrainer

# loop per worker script
loop_per_worker_script: training.loops.train_loop_per_worker

# training quantization
quantization: bfloat16 # data type for training, options: float32, float16

# NOTE: numpy doesn't support bfloat16, the preprocessor will cast data before we feed it to the model
dataset_dtype: float16 # data type for loading data, options: float32, float16
storage_dtype: uint16

# dataset layout in the zarr files
dataset_layout_order: ZYXC

# seed
seed: 42

# overrides
# --------------------------------------------------------------------------------

# clusters
clusters:
  batch_size: 64 # total batch size
  worker_nodes: 1 # number of worker nodes
  gpus_per_worker: 8 # number of gpus per worker node
  cpus_per_gpu: 12 # number of cpu cores per gpu
  mem_per_cpu: 16000 # ram per cpu core
  ray_queue_memory: 0.1

#paths:
paths:
  # base output directory for logs, checkpoints, etc.
  outdir: ${paths.data_path}/pretrained_models/${experiment_name}
  resume_checkpointdir: null
  pretrained_checkpointdir: null

# data
datasets:
  # mainly used by other configs but determined here for ease of access
  batch_size: ${clusters.batch_size_per_gpu} # batch size = (total_batch_size * total_gpus)
  input_shape: [128, 128, 128, 2]
  patch_shape: [16, 16, 16, null]
  occupancy_threshold: 0.9

  split: 0.1
  return_dataloader: true
  distributed_sampler: true

  buffer_capacity: 20
  num_actors_min: 4
  num_actors_max: 12

  # max_hypercubes: 50000
  max_hypercubes: 10000

  use_cached_hypercubes_dataframe: true
  hypercubes_dataframe_path: ${paths.server_folder_path}/databases/old_prepared_1_128_128_128_2_hypercube_view.csv

  collate_fn:
    columns:
      - "x_start"
      - "y_start"
      - "z_start"
      - "time_start"
      - "channel_size"
      - "z_size"
      - "y_size"
      - "x_size"
      - "time_size"
      - "server_folder"
      - "output_folder"
      - "tile_name"
      - "prepared_id"
      - "pc_metadata_json"

  databases:
    occupancy_threshold_filter_type: 'min_ch0'
    synthetic_only: true
    has_annotations: true
    server_folder_path: /groups/betzig/betziglab/CellObservatoryData/synthetic

# schedulers
schedulers:
  epochs: 2

# hooks
hooks:
  hooks_list:
  - _target_: training.hooks.WeightDecayScheduleHook
  - _target_: training.hooks.FreeDeviceBufferHook
  - _target_: training.hooks.AnomalyDetector
  - _target_: training.hooks.LRScheduler
  - _target_: training.hooks.IterationTimer
  - _target_: training.hooks.PeriodicWriter
  - _target_: training.hooks.PeriodicCheckpointer
    file_prefix: latest_model
  - _target_: training.hooks.BestCheckpointer
    checkpointdir: ${checkpoint.checkpoint_manager.save_checkpointdir}
  - _target_: training.hooks.TorchMemoryStats
    step_period: 1
    epoch_period: 1
    logdir: ${loggers.logdir}
  - _target_: training.hooks.BestMetricSaver
    metric_name: ${evaluation.val_metric}
    compare_fn: ${evaluation.val_mode}
    eval_after_validation: true