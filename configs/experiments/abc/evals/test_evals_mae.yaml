# @package _global_
defaults:
  - /tasks/channel_split
  - /paths/abc                               # configs/paths/abc.yaml
  - /hydra/overrides                         # configs/hydra/overrides.yaml (not working)
  - /clusters/local                          # configs/clusters/local.yaml
  - /datasets/pretrain_dataset_ray           # configs/datasets/pretrain_dataset.yaml
  - /datasets/preprocessor/channel_split_preprocessor  # configs/datasets/preprocessor/channel_split_preprocessor.yaml
  - /datasets/masking/mask_generator         # configs/datasets/masking/mask_generator.yaml    
  - /models/mae/large                        # configs/models/mae/large.yaml
  - /models/decoders/vit                     # configs/models/decoders/vit.yaml
  - /hooks/hooks                             # configs/hooks/hooks.yaml
  - /checkpoint/checkpoint                   # configs/checkpoint/checkpoint.yaml
  - /loggers/loggers                         # configs/loggers/loggers.yaml
  - /evaluation/channel_split_evaluator      # configs/evaluation/channel_split_evaluator.yaml
  - /deepspeed/zero2                         # configs/deepspeed/zero2.yaml
  - /optimizers/lamb                         # configs/optimizers/lamb.yaml
  - /schedulers/test_warmup_cosine_decay     # configs/schedulers/test_warmup_cosine_decay.yaml
  - /optimizations/optimizations             # configs/optimizations/optimizations.yaml
  - /profiling/profiling                      # configs/profiling/profiling.yaml
  - _self_

# experiment name (used for logging)
experiment_name: test_evals
wandb_project: test_evals

# task
task: pretrain_mae

# Model type
network: mae

# engine type
engine: deepspeed

# distributed framework
distributed_framework: ray

# run type
run_type: single_run # single_run, multi_run, or tune

# job type
job_type: test # train or test

# trainer type
trainer: training.loops.TestTrainer

# loop per worker script
loop_per_worker_script: training.loops.train_loop_per_worker

# training quantization
quantization: bfloat16 # data type for training, options: float32, float16

# NOTE: numpy doesn't support bfloat16, the preprocessor will cast data before we feed it to the model
dataset_dtype: float16 # data type for loading data, options: float32, float16
storage_dtype: uint16

# dataset layout in the zarr files
dataset_layout_order: ZYXC

# seed
seed: 42

data_dim: 3

# overrides
# --------------------------------------------------------------------------------

# clusters
clusters:
  ray_queue_memory: 0.2
  batch_size: 16 # total batch size
  worker_nodes: 1 # number of worker nodes
  gpus_per_worker: 1 # number of gpus per worker node
  cpus_per_gpu: 16 # number of cpu cores per gpu
  mem_per_cpu: 21000 # ram per cpu core
  scaling_config:
    use_gpu: true
    num_workers: ${clusters.total_gpus} # number of total workers (equals total nr. gpus normally)
    resources_per_worker:
      CPU: 1
      GPU: 1
    trainer_resources:
      CPU: 0 

# checkpoint
checkpoint:
  checkpoint_manager:
    checkpoint_tag: latest_model

#paths:
paths:
  # base output directory for logs, checkpoints, etc.
  outdir: ${paths.data_path}/pretrained_models/${experiment_name}
  resume_checkpointdir: null 
  pretrained_checkpointdir: /clusterfs/nvme/segment_4d/pretrained_models/test_channel_split_vit/checkpoints

# data
datasets:
  # mainly used by other configs but determined here for ease of access
  batch_size: ${clusters.batch_size_per_gpu} # batch size = (total_batch_size * total_gpus)
  input_shape: [128, 128, 128, 2] # NOTE: important that this matches database hypercube shape
  patch_shape: [null, 16, 16, 16]

  databases:
    max_rois: null # maximum number of ROIs (each ROI can have dozens of tiles)
    max_tiles: null # maximum number of tiles (each tile can have thousands of hypercubes)
    roi_list: [516, 517, 518, 519]
    max_hypercubes: 500 # maximum number of hypercubes to return
    use_cached_hypercubes_dataframe: true # whether to use the cached hypercubes dataframe
    hypercubes_dataframe_path: /clusterfs/nvme/segment_4d/pretrained_models/test_channel_split_linear/database/hypercubes_dataframe.csv
    # server_folder_path: /clusterfs/vast/Data/read_speed_tests/large_test_zarrs
    protocol: cursor # protocol used to fetch data from source (default is 'binary', can also be 'csv' or 'cursor')
    num_timepoints: 1

# schedulers
schedulers:
  epochs: 1

# deepspeed
deepspeed:
  checkpoint:
    load_universal: true # whether to load a universal checkpoint

# hooks
hooks:
  # nsys_env:
  #   nsight: 
  #     pytorch: autograd-shapes-nvtx
  #     t: cuda,nvtx,cudnn,cublas,osrt
  #     s: cpu
  #     # "gpu-metrics-devices": 0
  #     capture-range: cudaProfilerApi
  #     capture-range-end: stop-shutdown
  hooks_list:
  # - _target_: training.hooks.NsysProfilerHook
  #   start_iter: 20
  #   end_iter: 25
  - _target_: training.hooks.FreeDeviceBufferHook
  - _target_: training.hooks.IterationTimer
  - _target_: training.hooks.PeriodicWriter
  - _target_: training.hooks.TorchMemoryStats
    step_period: 2
    epoch_period: 1
    logdir: ${loggers.logdir}