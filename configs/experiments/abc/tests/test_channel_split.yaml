# @package _global_
defaults:
  - /tasks/channel_split
  - /paths/abc                               # configs/paths/abc.yaml
  - /hydra/overrides                         # configs/hydra/overrides.yaml (not working)
  - /clusters/local                          # configs/clusters/local.yaml
  - /datasets/pretrain_dataset_ray           # configs/datasets/pretrain_dataset.yaml
  - /datasets/preprocessor/channel_split_preprocessor  # configs/datasets/preprocessor/channel_split_preprocessor.yaml
  - /datasets/masking/mask_generator         # configs/datasets/masking/mask_generator.yaml
  - /models/BUILD_AutoBench                  # configs/models/BUILD_AutoBench.yaml
  - /models/meta_arch/autobench/ChannelSplitAutoBench/mae_large  # configs/models/meta_arch/autobench/ChannelSplitAutoBench/mae_large.yaml
  - /models/backbones/mae/large              # configs/models/mae/large.yaml
  - /models/decoders/linear                  # configs/models/decoders/linear.yaml
  - /hooks/hooks                             # configs/hooks/hooks.yaml
  - /checkpoint/checkpoint                   # configs/checkpoint/checkpoint.yaml
  - /loggers/loggers                         # configs/loggers/loggers.yaml
  - /evaluation/base_evaluator               # configs/evaluation/base_evaluator.yaml
  - /deepspeed/zero2                         # configs/deepspeed/zero2.yaml
  - /optimizers/lamb                         # configs/optimizers/lamb.yaml
  - /schedulers/test_warmup_cosine_decay     # configs/schedulers/test_warmup_cosine_decay.yaml
  - /optimizations/optimizations             # configs/optimizations/optimizations.yaml
  - /profiling/profiling                      # configs/profiling/profiling.yaml
  - _self_

# experiment name (used for logging)
experiment_name: test_channel_split_vit
wandb_project: testing

# task
task: pretrain_mae

# Model type
network: mae

# engine type
engine: deepspeed

# distributed framework
distributed_framework: ray

# run type
run_type: single_run # single_run, multi_run, or tune

# job type
job_type: train # train or test

# trainer type
trainer: training.loops.EpochBasedTrainer

# loop per worker script
loop_per_worker_script: training.loops.train_loop_per_worker

# training quantization
quantization: bfloat16 # data type for training, options: float32, float16

# NOTE: numpy doesn't support bfloat16, the preprocessor will cast data before we feed it to the model
dataset_dtype: float16 # data type for loading data, options: float32, float16
storage_dtype: uint16

# dataset layout in the zarr files
dataset_layout_order: ZYXC

# seed
seed: 42

# overrides
# --------------------------------------------------------------------------------

#paths:
paths:
  # base output directory for logs, checkpoints, etc.
  outdir: ${paths.data_path}/pretrained_models/${experiment_name}
  resume_checkpointdir: null 
  pretrained_checkpointdir: null

# clusters
clusters:
  ray_queue_memory: 0.2
  batch_size: 1 # total batch size
  worker_nodes: 1 # number of worker nodes
  gpus_per_worker: 1 # number of gpus per worker node
  cpus_per_gpu: 16 # number of cpu cores per gpu
  mem_per_cpu: 21000 # ram per cpu core

# data
datasets:
  # mainly used by other configs but determined here for ease of access
  batch_size: ${clusters.batch_size_per_gpu} # batch size = (total_batch_size * total_gpus)
  train_shape: [128, 128, 128, 1]
  input_shape: [128, 128, 128, 2]
  patch_shape: [16, 16, 16, null]
  occupancy_threshold: 0.9

  split: 0.1
  return_dataloader: true
  distributed_sampler: true

  buffer_capacity: 20
  num_actors_min: 4
  num_actors_max: 4

  # max_hypercubes: 50000
  max_hypercubes: null
  max_rois: 1 # maximum number of ROIs (each ROI can have dozens of tiles)
  max_tiles: null # maximum number of tiles (each tile can have thousands of hypercubes)

  use_cached_hypercubes_dataframe: true
  hypercubes_dataframe_path: /clusterfs/nvme/segment_4d/databases/prepared_1_128_128_128_2_hypercube_view_NEW.csv

  databases:
    occupancy_threshold_filter_type: 'min_ch0'
    synthetic_only: true
    has_annotations: true
    with_hypercubes_dataframe: true
    server_folder_path: /clusterfs/vast/forsynthetic/benchmark_tests/data/synthetic_data_iteration_1
    mask_channel: 1

# schedulers
schedulers:
  # epochs: 2
  epochs: 1

# hooks
hooks:
  hooks_list:
  - _target_: training.hooks.WeightDecayScheduleHook
  - _target_: training.hooks.FreeDeviceBufferHook
  - _target_: training.hooks.AnomalyDetector
  - _target_: training.hooks.LRScheduler
  - _target_: training.hooks.IterationTimer
  - _target_: training.hooks.PeriodicWriter
  - _target_: training.hooks.PeriodicCheckpointer
    file_prefix: latest_model
  - _target_: training.hooks.BestCheckpointer
    checkpointdir: ${checkpoint.checkpoint_manager.save_checkpointdir}
  - _target_: training.hooks.TorchMemoryStats
    step_period: 1
    epoch_period: 1
    logdir: ${loggers.logdir}
  - _target_: training.hooks.BestMetricSaver
    metric_name: ${evaluation.val_metric}
    compare_fn: ${evaluation.val_mode}
    eval_after_validation: true