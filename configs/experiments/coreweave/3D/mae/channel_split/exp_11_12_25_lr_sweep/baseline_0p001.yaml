# @package _global_
defaults:
  - /tasks/channel_split
  - /paths/coreweave                         # configs/paths/coreweave.yaml
  - /hydra/overrides                         # configs/hydra/overrides.yaml (not working)
  - /clusters/coreweave_h100                 # configs/clusters/coreweave_h100.yaml
  - /datasets/pretrain_dataset_ray           # configs/datasets/pretrain_dataset.yaml
  - /datasets/preprocessor/channel_split_preprocessor  # configs/datasets/preprocessor/channel_split_preprocessor.yaml
  - /datasets/masking/mask_generator         # configs/datasets/masking/mask_generator.yaml  
  - /models/mae/large                        # configs/models/mae/large.yaml
  - /models/decoders/vit                     # configs/models/decoders/vit.yaml
  - /hooks/hooks_with_weight_decay           # configs/hooks/hooks.yaml
  - /checkpoint/checkpoint                   # configs/checkpoint/checkpoint.yaml
  - /loggers/loggers                         # configs/loggers/loggers.yaml
  - /evaluation/base_evaluator               # configs/evaluation/base_evaluator.yaml
  - /deepspeed/zero2                         # configs/deepspeed/zero2.yaml
  - /optimizers/lamb_mae                     # configs/optimizers/lamb.yaml
  - /schedulers/warmup_cosine_decay_with_weight_decay # configs/schedulers/test_warmup_cosine_decay.yaml
  - /optimizations/optimizations             # configs/optimizations/optimizations.yaml
  - /profiling/profiling                     # configs/profiling/profiling.yaml
  - _self_

# experiment name (used for logging)
experiment_name: mae_large_128x256x256_100_epoch_baseline_channel_split_lr_0p001
wandb_project: testing #finetuning_3d

# task
task: pretrain_mae

# Model type
network: mae

# engine type
engine: deepspeed

# distributed framework
distributed_framework: ray

# run type
run_type: single_run # single_run, multi_run, or tune

# job type
job_type: train # train or test

# trainer type
trainer: training.loops.EpochBasedTrainer

# loop per worker script
loop_per_worker_script: training.loops.train_loop_per_worker

# training quantization
quantization: bfloat16 # data type for training, options: float32, float16

# NOTE: numpy doesn't support bfloat16, the preprocessor will cast data before we feed it to the model
dataset_dtype: float16 # data type for loading data, options: float32, float16
storage_dtype: uint16

# dataset layout in the zarr files
dataset_layout_order: ZYXC

# seed
seed: 42

# overrides
# --------------------------------------------------------------------------------

# clusters
clusters:
  batch_size: 256 # total batch size
  worker_nodes: 2 # number of worker nodes
  gpus_per_worker: 8 # number of gpus per worker node
  cpus_per_gpu: 12 # number of cpu cores per gpu
  mem_per_cpu: 40000 # ram per cpu core
  ray_queue_memory: 0.1

# checkpoint
# checkpoint:
#   checkpoint_manager:
#     checkpoint_tag: latest_model

#paths:
paths:
  # base output directory for logs, checkpoints, etc.
  outdir: ${paths.data_path}/pretrained_models/${experiment_name}
  resume_checkpointdir: null
  pretrained_checkpointdir: null

# data
datasets:
  # mainly used by other configs but determined here for ease of access
  batch_size: ${clusters.batch_size_per_gpu} # batch size = (total_batch_size * total_gpus)
  input_shape: [128, 256, 256, 2]
  patch_shape: [16, 16, 16, null]
  occupancy_threshold: 0.9

  split: 0.1
  return_dataloader: true
  distributed_sampler: true

  buffer_capacity: 20
  num_actors_min: 4
  num_actors_max: 12

  max_hypercubes: 1000000

  use_cached_hypercubes_dataframe: true
  hypercubes_dataframe_path: ${paths.server_folder_path}/databases/prepared_1_128_128_128_2_hypercube_view.csv

  databases:
    occupancy_threshold_filter_type: 'min_ch0'

# schedulers
schedulers:
  epochs: 100

# schedulers
schedulers:
  epochs: 2

# ROPE axial
models:
  mae:
    abs_sincos_enc: false
    rope_pos_enc: true
    rope_random_rotation_per_head: true
    rope_mixed: false
    rope_theta: 100.0

optimizers:
  wd:  ${eval:'${optimizers.lr} * 0.1'}
  lr: 1e-3

# hooks
hooks:
  hooks_list:
  - _target_: training.hooks.WeightDecayScheduleHook
  - _target_: training.hooks.FreeDeviceBufferHook
  - _target_: training.hooks.AnomalyDetector
  - _target_: training.hooks.LRScheduler
  - _target_: training.hooks.IterationTimer
  - _target_: training.hooks.PeriodicWriter
  - _target_: training.hooks.PeriodicCheckpointer
    file_prefix: latest_model
  - _target_: training.hooks.BestCheckpointer
    checkpointdir: ${checkpoint.checkpoint_manager.save_checkpointdir}
  - _target_: training.hooks.TorchMemoryStats
    step_period: 1
    epoch_period: 1
    logdir: ${loggers.logdir}
  - _target_: training.hooks.BestMetricSaver
    metric_name: ${evaluation.val_metric}
    compare_fn: ${evaluation.val_mode}
    eval_after_validation: true