defaults:
  - models: maskrcnn_fpn        # Loads configs/models/maskrcnn_fpn.yaml
  - models/backbones: hiera_fpn     # Loads configs/models/backbones/hiera_fpn.yaml
  - datasets: instance_segmentation       # Loads configs/datasets/instance_segmentation.yaml
  - transforms: transforms_skittlez # Loads configs/transforms/transforms_skittlez.yaml
  - optimizers: adamw # Loads configs/optimizers/adamw.yaml
  - schedulers: warmup_cosine_decay # Loads configs/schedulers/warmup_cosine_decay.yaml
  - metrics: metrics        # Loads configs/metrics/metrics.yaml
  - deepspeed: deepspeed # Loads configs/deepspeed/deepspeed.yaml
  - clusters: local # Loads configs/clusters/slurm_multinode.yaml
  - _self_  # self reference last to allow for overrides

# overrides
# ----------------

# backbone (TODO: not technically an override, but probably should be) 
# downsampling resolution example for 256x256x256 input: 
# 256 -> 64 (3d conv) -> 64 (q0) -> 32 (q1) -> 16 (q2) -> 8 (q3) 
# (which is why we skip q3 currently) 
backbone_target: finetune.models.backbones.hiera.Hiera
backbone_out_channels: 256 # 256 is standard from FPN paper 
# featmaps used for RPN and ROI pooling heads
output_features: ["p0", "p1", "p2"] # "p3", "pool" (p^i downsamples starting shape by 2^i+1)

models:
  backbones:
    _target_: finetune.models.backbones.fpn.BackboneWithFPN
    # NOTE: FPN may add an extra LastLevelMaxPool layer to the last feature map
    # Hence, the output feature maps may have len(return_layers) + 1 maps
    # return layers specifies which feature maps to run through the FPN
    return_layers: {"p0": "p0", "p1": "p1", "p2": "p2"} # 2x downsample per return layer used + LastLevelMaxPool
    in_channels_list: [96, 192, 384] # , 768  (matches embed_dim * stage sequence)
    out_channels: ${backbone_out_channels}
    norm_layer: null
    # extra_blocks # FPN may add an extra LastLevelMaxPool layer to the last feature map
  mask_roi_pool:
    _target_: finetune.models.utils.poolers.MultiScaleRoIAlign
    featmap_names: ${output_features}
    output_size: 14 # ROI pooling output size
    sampling_ratio: 2 # grid subsampling 
    canonical_scale: 256
    canonical_level: 4 # levels are 2,3,4 i.e. 2^2, 2^3, 2^4 downsampling of input
  rpn_anchor_generator:
    _target_: finetune.models.utils.proposal_generators.anchor_generator.AnchorGenerator
    sizes:
      - [16, 24, 32]
      - [32, 48, 64]
      - [64, 96, 128]
      # - [128, 196, 256]
    aspect_ratios:
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      # - [0.5, 1.0, 2.0]
    aspect_ratios_z:
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      - [0.5, 1.0, 2.0]
      # - [0.5, 1.0, 2.0]
  rpn_head: 
    _target_: finetune.models.utils.proposal_generators.rpn_head.RPNHead
    in_channels: ${backbone_out_channels} 
    num_anchors: 27 # 3 sizes * 3 aspect ratios * 3 aspect ratios_z (TODO: make this dynamic)
  
  # NOTE: consider chaning the number of positive samples considered if the model is not learning
  #       and only predicts background
  # rpn_fg_iou_thresh: 0.5 # too high of a value will cause the model to not learn anything and always predict the background
  # rpn_bg_iou_thresh: 0.3
  # box_positive_fraction: 0.25 # positive fraction of samples in subsampling

clusters:
  batch_size: 1
  total_cpus:               4          # total number of cpus to use
  total_gpus:               1          # total number of gpus to use
  gpus_per_worker:          1          # number of gpus to use per node
  mem_per_worker:           31000      # memory per node
  cpus_per_worker:          4          # number of cpus to use per node  
  workers:                  1          # number of nodes to use 

transforms:
  transforms_list:
    - _target_: finetune.data.transforms.transforms.Normalize
      # mean: ${dataset_mean}
      # std: ${dataset_std}
    - _target_: finetune.data.transforms.transforms.Resize
      size: 
        # - ${datasets.channel_in}
        - 256
        - 256 
        - 256

# ----------------

# decoder type (not used)
network: maskrcnn

# training paradigm
paradigm: finetune.train.backend_segmentation.supervised

# paths
outdir: /clusterfs/nvme/segment_4d/test_16
logdir: ${outdir}/logs

load_checkpointdir: null # alternative checkpointdir to load ckpt from if checkpointdir in outdir is empty 
checkpointdir: ${outdir}/checkpoints
checkpoint_update_interval: 100 # save latest checkpoint every N epochs

val_interval: 1
log_step: 100 # step interval to show training loss in train loop
val_log_step: 20 # step interval to show loss in val loop

train_db_savedir: ${outdir}/db
db_path: /clusterfs/nvme/segment_4d/final_pipeline_v3/db/segmentation_curation_test.db