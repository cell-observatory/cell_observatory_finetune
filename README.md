# 3D Segmentation Models

3D segmentation models designed specifically for 3D Light Sheet (LS) microscopy datasets. 
Built with [PyTorch](https://pytorch.org/), accelerated and scaled with [Ray](https://www.ray.io/), and flexibly configured using [Hydra](https://hydra.cc/).


## Table of Contents

- [Installation](#installation)
- [Get Started](#get-started)
  - [Local Setup](#local-setup)
    - [Training](#training)
    - [Evaluation](#evaluation)
  - [Cluster Setup](#cluster-setup)
    - [Training](#training-1)
- [Model Configurations](#configurations)
- [Data Pipeline](#data-pipeline)
  - [Structures](#structures)
    - [Data Objects](#data-objects)
    - [Sample Objects](#sample-objects)
  - [Databases](#databases)
  - [Datasets](#datasets)
- [Models](#background-and-architecture)
  - [Base Model](#base-model)
  - [Backbones](#backbones)
    - [ResNet](#resnet-overview)
    - [ConvNeXt](#convnext-overview)
    - [InternImage](#internimage-overview)
    - [ViTDet](#vitdet-overview)
    - [Hiera](#hiera-overview) 
    - [FPN](#fpn)
  - [Heads](#heads)
    - [Mask R-CNN](#mask-r-cnn-overview)
    - [MASK DINO](#mask-dino-overview)
- [Evaluators](#evaluators)
- [Acknowledgements](#acknowledgements)

## Installation

### Conda
This package has been tested on **CUDA 12.4** with **PyTorch 2.4.1** and **Python 3.10**. We recommend using a dedicated `conda` environment.

```bash
# 1. Create & activate conda environment
conda create -n segmentation python=3.10
conda activate segmentation

# 2. Clone the repository
git clone --recurse-submodules https://github.com/cell-observatory/segmentation.git
cd segmentation

# 3. Install ops3d submodule
cd ops3d
pip install .

# 4. Install package in editable mode
cd .. 
pip install -e . 
```

### Docker and Apptainer

Prebuilt image is available on our [GitHub container registry](https://github.com/cell-observatory/platform/pkgs/container/platform).
```shell
docker pull ghcr.io/cell-observatory/platform:develop_torch_cuda_12_8
```

To run our docker image, replace `$(pwd)` with your local path for the repo and run the following command:
```shell
docker run --network host -u 1000 --privileged -v $(pwd):/app/segmentation -w /app/segmentation --env PYTHONUNBUFFERED=1 --pull missing -it --rm  --ipc host --gpus all ghcr.io/cell-observatory/platform:develop_torch_cuda_12_8 bash
```

You'll need an Apptainer version of the image to run it on cluster, which can be generated by:
```shell
apptainer pull --force develop_torch_cuda_12_8.sif docker://ghcr.io/cell-observatory/platform:develop_torch_cuda_12_8
```


## Get Started

### Local Setup

#### Training

Run training locally by specifying a Hydra config file (see [Model Configurations](#configurations) for details on how to configure experiments using Hydra YAML files):

```bash
python3 training/train_segmentation.py --config-name=config_mrcnn_vitDet.yaml
```

Or run via SLURM scheduler using the provided bash script (edit the slurm bash file as needed):

```bash
sbatch training/train_segmentation.sh --config-name=config_mrcnn_vitDet.yaml
```

#### Evaluation

Run evaluation locally by specifying a Hydra config file:

```bash
python3 evaluation/evaluate.py --config-name=skittlez_evaluation_vitDet.yaml
```

Or run via SLURM scheduler using the provided bash script (edit the slurm bash file as needed):

```bash
sbatch evaluation/evaluate.sh --config-name=skittlez_evaluation_vitDet.yaml
```

### Cluster Setup

#### Training

#### Cluster Setup — Ray-on-SLURM

The Ray-on-SLURM cluster config lives in **`configs/clusters/slurm_multinode.yaml`**. Edit just the handful of lines below, then launch the job.

#### 1. Edit `slurm_multinode.yaml`

```yaml
# ── JOB-SPECIFIC PATHS ──────────────────────────────────────────────
head_node_script: /ABS/PATH/TO/clusters/ray_slurm_cluster_job.sh   # SLURM head-node launcher
workers_script:   /ABS/PATH/TO/clusters/ray_slurm_workers.sh       # SLURM worker launcher
script:           /ABS/PATH/TO/train/train_segmentation.py         # Python train entry point
mount_path:       /ABS/PATH/TO/PROJECT/DATA                        # host path (bind mount)
src:              /ABS/PATH/TO/segmentation/src/segmentation       # repo root inside image
apptainer:        /ABS/PATH/TO/my_apptainer_image.sif              # container image
ray:              /ABS/PATH/TO/clusters                            # folder with Ray cluster helpers

# ── RESOURCE REQUESTS ──────────────────────────────────────────────
partition:          gpu_a100          # SLURM partition
qos:                high              # QoS
workers:            3                 # number of nodes
gpus_per_worker:    4                 # GPUs per node
cpus_per_worker:    4                 # CPUs per gpu worker
mem_per_worker:     31G               # Mem per gpu worker

# Optional overrides
exclusive:   true          # true → whole nodes; false → shared nodes
...
```

#### 2. Submit Ray-on-SLURM jobs with `train_segmentation_multinode.sh`
We provide a convenience wrapper script that picks the Hydra config and submits the Ray-on-SLURM job:

```bash
# Set config and then run:
bash src/segmentation/training/train_segmentation_multinode.sh
```

## Model Configurations with Hydra

We use [Hydra](https://hydra.cc/) for managing experiment configurations. Hydra allows you to construct experiments by composing modular YAML files.

### 1. Select Base Configurations

Use the `defaults:` list to select the base YAML configurations for your experiment:

```yaml
defaults:
  - models:           maskrcnn_fpn
  - models/backbones: hiera_fpn
  - datasets:         skittlez
  - transforms:       transforms_skittlez
  - metrics:          metrics
  - _self_            # load this file’s overrides last
```

### 2. Override Only What You Need

Hydra handles overrides, allowing precise experiment adjustments. Scalars and lists are replaced outright, whereas dictionaries are merged recursively (only specified keys change).

Example overrides in your main config file:

```yaml
# Change batch size
datasets:
  batch_size: 4

# Swap out backbone model
backbone_target: segmentation.models.backbones.resnet.ResNet
backbone_out_channels: 2048

# Adjust RPN anchor sizes
models:
  rpn_anchor_generator:
    sizes:
      - [16, 32, 64]
      - [32, 64, 128]
```

### 3. Configuration Directory Structure & Usage

Here's what each configuration subdirectory handles:

- **`models/`**
  - Defines complete model specifications (e.g., Mask R‑CNN variants)
- **`models/preprocessors/`**
  - Defines preprocessing modules for model inputs
- **`models/backbones/`**
  - Defines backbone networks and corresponding FPN wrappers
- **`models/heads/`**
  - Defines detection and segmentation heads (e.g., Mask R-CNN, DINO)
- **`models/matchers/`**
  - Defines matching algorithms for detection and segmentation tasks
- **`datasets/`**
  - Defines dataset classes and parameters
- **`losses/`**
  - Defines loss functions used in training
- **`transforms/`**
  - Defines data augmentation and preprocessing pipelines and parameters
- **`optimizers`**
  - Defines optimizer configurations (e.g., AdamW, SGD)
- **`schedulers/`**
  - Defines learning rate schedulers (e.g., StepLR, CosineAnnealing)
- **`metrics/`**
  - Defines evaluation metrics computed during validation by our custom `Evaluator` class
- **`deepspeed/`**
  - Defines DeepSpeed configurations for distributed training
- **`clusters/`**
  - Defines cluster configurations for distributed training (e.g., Ray-on-SLURM)

### 4. Extend or Add New Modules

Add a new YAML under the proper group, e.g. `configs/models/backbones/my_new_backbone.yaml`. To support a brand‑new model or dataset, just drop in your new small YAML and reference it in your `defaults:` block.

```yaml
defaults:
  - models/backbones: my_new_backbone
  ...
  ...
  ...
  - _self_
```


## Data Pipeline

### Structures

#### Data Objects

• **`ImageList:`** A tensor container that holds images of varying sizes as a single padded tensor, storing original image sizes, layout information (`CZYX` vs `ZYXC` etc.), and providing methods for standardization and batch operations.

• **`Boxes:`** A class that wraps bounding box tensors with methods for geometric operations like resizing, clipping, and coordinate transformations.

• **`Labels:`** A tensor wrapper for classification labels that maintains class count information and provides utilities for label manipulation.

• **`BitMasks:`** A class for storing and manipulating binary segmentation masks with methods for resizing, cropping, and boolean operations.

#### Sample Objects

• **`BaseDataElement:`** A base class that provides dict-like and tensor-like operations for data containers, separating metainfo (image metadata) from data fields (annotations/predictions).

• **`DataSample:`** Inherits from `BaseDataElement` for instance segmentation tasks, containing an `ImageList` for image tensors and `Instances` objects for annotations. Contains class methods `from_dict` and `to_dict` for serialization and deserialization.

• **`Instances:`** A container class that stores masks, boxes, and labels as structured objects, providing indexing, slicing, and concatenation methods while ensuring dimensional consistency across all fields.

### Databases

**`Database (Abstract):`** Base class defining `_fetch()` method for querying any database table and `_init_db()` method for creating local preprocessed DataFrames with metadata including image paths, tile sizes, acquisition metadata, reagents metadata, quality metrics, and sample metadata.

**`SQLiteDatabase:`** Concrete implementation using schema-based querying to build SQL queries from configuration files, then tiles images according to specified dimensions to create local DataFrames where each row represents an image tile with associated metadata.

**`DB_DataSchema:`** Configuration class containing dictionaries for different metadata types (`data_attributes`, `quality_metrics`, `sample_metadata`, `reagents_metadata`, `acquisition_metadata`) where each entry is a `(key, comparator)` tuple for building `SQL WHERE` clauses.
```python
@dataclass
class DB_DataSchema:
    table: str 
    data_attributes: Dict[str, Any] = None
    quality_metrics: Optional[Dict[str, Any]] = None
    sample_metadata: Optional[Dict[str, Any]] = None
    reagents_metadata: Optional[Dict[str, Any]] = None
    accquisition_metadata: Optional[Dict[str, Any]] = None
```
**`DB_LabelSchema:`** Similar schema class for label tables containing `label_metadata` dictionary with `(key, comparator)` tuples for filtering annotation data.
````python
@dataclass
class DB_LabelSchema:
    table: str                                
    label_metadata: Optional[Dict[str, Any]] = None
````
**`Comparators:`** Enum defining SQL comparison operators (EQUAL, GREATER_THAN, IN, BETWEEN, etc.) used in schema specifications to build conditional queries for metadata filtering and thresholding.

### Datasets

**`BaseDataset:`** Abstract `PyTorch` dataset class that takes local data tables from `Database` class and creates fast indexes through four key methods: `_process_tables()` filters data, `_build_index()` converts DataFrames to list of dicts for `O(1)` access, `_load_sample()` reads image data using zarr handles, and `_collate()` converts raw data into structured objects.

**`InstanceSegDataset:`** Concrete implementation for 3D instance segmentation that maintains zarr file handles for efficient I/O, builds multiindex from label tables, loads image crops and masks using coordinate slicing, then packages data into `DataSample` objects containing `ImageList` tensors and `Instances` with `boxes`, `masks`, and `labels`.

**`Compose:`** Transform pipeline class that sequentially applies data augmentations and preprocessing transforms to `DataSample` objects, enabling modular and configurable data preprocessing workflows.

## Models

### Base Model

**`BaseModel:`** Abstract `PyTorch` module that all segmentation models inherit from, containing a `PreProcessor` module for converting dictionary data to structured `DataSample` objects before calling the abstract `_forward()` method.

**`PreProcessor:`** Module that converts raw dictionary data into specified sample object types (`DataSample` by default) using the `from_dict()` method, standardizing input format across different model architectures.

### Backbones

#### ResNet Overview

Adapted from Torchvision’s ResNet [1], our 3D‑ResNet replaces all 2D operations with their 3D counterparts (`nn.Conv3d`, `nn.BatchNorm3d`, `nn.MaxPool3d`, etc.).

- **Stem**  
  - **Conv3d:** `out_channels = 64`, `kernel = 7³`, `stride = 2`, `padding = 3`  
  - **BatchNorm3d** 
  - **ReLU** 
  - **MaxPool3d:** `kernel = 3`, `stride = 2`, `padding = 1`  

- **Residual Stages**  
  Each stage consists of a `block` module (either **Basic** or **Bottleneck** blocks). Depending on volume size, the number of layers used may vary. 
  1. **Stage p2**   
     Output: `(64·expansion, D/4, H/4, W/4)`  
  2. **Stage P3**   
     Output: `(128·expansion, D/8, H/8, W/8)`  
  3. (Optional) **Stage p4**   
     Output: `(256·expansion, D/16, H/16, W/16)`  
  4. (Optional) **Stage p5**   
     Output: `(512·expansion, D/32, H/32, W/32)`

- **Block Types**  
  - **BasicBlock** (`expansion = 1`):  
    ```python
    # Pseudocode
    out  =  ReLU(BN3D(conv3x3(x)))
    out  =  BN3D(conv3x3(out))
    out +=  x
    out  =  ReLU(out)
    ```  
    
  - **Bottleneck** (`expansion = 4`):  
    ```python
    # Pseudocode
    out  = RELU(BN3D(conv1x1(x)))
    out  = ReLU(BN3D(conv3x3(out)))
    out  = BN3D(conv1x1(out))
    out += x
    out  = ReLU(out)
    ```  

#### ConvNeXt Overview

Adapted from FAIR's `ConvNetV2`, our `3D-ConvNeXt` replaces all 2D operations with their 3D counterparts (`nn.Conv3d`, etc.).

#### InternImage Overview

Adapted from OpenGVLab's `DCNv4`, our `3D-InternImage` replaces all 2D operations with their 3D counterparts, including deformable convolution kernels that can be found in the `ops3d` submodule.

#### ViTDet Overview

Adapted from Detectron2’s VitDet implementation [3], our 3D-ViTDet replaces any 2D operations with their 3D counterparts:

- **Patch Embedding**  
  - **Conv3d:** `out_channels = embed_dim`, `kernel_size = (patch_size)³`, `stride = (patch_size)³`  

- **Absolute Positional Embeddings**  
  - Optional **absolute** positional embeddings

- **VitDet Transformer Block** 
    ```python
    # Pseudocode
    shortcut = x
    x = norm1(x)
    if window_size > 0:
        x = window_partition(x)
    x = attn(x)
    if window_size > 0:
        x = window_unpartition(x)
    x = shortcut + drop_path(x)
    x = x + drop_path(mlp(norm2(x)))
    if use_residual_block:
        x = residual(x)
    ```
  
- **Simple Feature Pyramid**  
  Builds `{p2, p3, …}` from `"last_feat"` returned  by the ViT backbone through upsamling/downsampling with `scale factors` (e.g. 4×, 2×, 1×):
  1. **Upsample/Downsample**  
     - `ConvTranspose3d` for `scale>1`  
     - `MaxPool3d` for `scale<1`  
  2. **Channel Unify**  
     - `Conv3d` layers (1×1×1 → 3×3×3 ) to set each map to `out_channels`  

#### Hiera Overview

Taken from “Hiera: A Hierarchical Vision Transformer without the Bells‑and‑Whistles” [3]:

- **Patch Embedding**  
  - **Conv3d:** `out_channels = embed_dim`, `kernel_size = (patch_kernel)³`, `stride = (patch_stride)³`
    - Zeros-out masked regions before `Conv3d` to prevent leakage of masked regions when using overlapping kernels

- **Positional Embedding**  
  - Learned `positional embeddings`, optionally decomposed across spatial dimensions (Z, Y, X)

- **Unroll & Reroll**  
  - **`Unroll`** Reorders the tokens such that patches are contiguous in memory (e.g. `[B, (D, H, W), C] -> [B, (Sz, Sy, Sx, D // Sz, H // Sy, W // Sx), C]`).
  - **`Reroll`** undos `unroll` operation at stage end to recover original spatial order for intermediate feature maps.

- **HieraBlock**  
 ```python
    # Pseudocode
    x_norm = norm1(x)
    if dim != dim_out:
        # query pooling
        x = do_pool(proj(x_norm), stride=q_stride)
    x = x + drop_path(attn(x_norm))
    x = x + drop_path(mlp(norm2(x)))
 ```

- **Hierarchical Stages**  
  - After each stage (e.g. `stages=[2, 3, 16]` Hiera Blocks), set `new_embed_dim = embed_dim * dim_mul` and `new_num_heads = num_heads * head_mul`.  
  - If `return_intermediates=True`, applies `Reroll` at each stage end to yield feature maps of shape `(B, embed_dim_intermediate, D_intermediate, H_intermediate, W_intermediate)`.

#### FPN Overview

We optionally combine each 3D backbone with a Feature Pyramid Network [5], adapted from Torchvision's 2D FPN by replacing all 2D operations with their 3D counterparts:

- **Return Layers**  
  Select intermediate outputs (e.g. `"p2"`, `"p3"`) to be used to build the pyramid with `return_layers`.
- **FPN**
  - **Lateral Conv3d:** `kernel_size = 1³`, `stride = 1³`, `padding = 0`
  - **Top‑Down Pathway:** `F.interpolate(coarse_map, mode="nearest") + lateral map`  
  - **Merge Conv3d:** `kernel_size = 3³`, `stride = 1³`, `padding = 1`

All 3D‑FPN outputs are returned as an `OrderedDict[str, Tensor]` of multi‑scale feature maps, for downstream detection or segmentation heads.  

### Heads

#### Mask R-CNN Overview

Adapted from TorchVision’s Mask R‑CNN, our 3D mask R-CNN replaces all 2D operations with their 3D counterparts. All custom CUDA kernels are very lightly adapted from https://github.com/TimothyZero/MedVision. 

- **GeneralizedRCNNTransform**:  
  1. **Resize** image to range `[min_size, max_size]`  
  2. **Normalize** per-channel with `image_mean` and `image_std`  
  3. **Pad & batch** into `ImageList` data structure

- **Backbone**  
  - Any `backbone` above, returning either a single `(B, C_out, D′, H′, W′)` tensor or an `OrderedDict[str, Tensor]` of multi‑scale feature maps.

- **Region Proposal Network (RPN)**  
  1. **AnchorGenerator** generates 3D anchors for each `(D,H,W)` with specified `anchor_sizes`, `aspect_ratios`, and `aspect_ratios_z`
  2. **RPNHead:**  
     - **Conv3dNormActivation:** repeated `conv_depth` times, `kernel_size = 3`, `stride = 1`
     - **Objectness Score Conv3d:** `kernel_size = 1`, `stride = 1`, `out_channels = num_anchors`   
     - **Box Deltas Conv3d**: `kernel_size = 1`, `stride = 1`, `out_channels = 6*num_anchors`
  3. **Decode & Filter** with `BoxCoder.decode`, clip to volume bounds, discard small/low‑score boxes, and apply NMS

- **ROI Heads**  
    **Detection Branch**
    1. **MultiScaleRoIAlign:** Leverages feature maps `featmap_names`, with `output_size=7` and `sampling_ratio=2` to align and resize each ROI proposal from the RPN to the optimal feature map in the feature map hierarchy.  
    2. **TwoMLPHead**:  
        ```python    
            # Pseudocode
            x = flatten(x, start_dim=1)       # [b, ch·d·h·w]
            x = relu(fc6(x))                  # first fully‐connected + activation
            x = relu(fc7(x))                  # second fully‐connected + activation
          ```
    3. **FastRCNNPredictor**:  
        ```python
            # Pseudocode
            scores      = cls_score(x)                 # returns: [b, num_classes]
            bbox_deltas = bbox_pred(x)                 # returns: [b, num_classes * 6]
        ```
    **Mask Branch**  
    Extends Faster R‑CNN with a parallel mask prediction head:
    1. **MultiScaleRoIAlign**: Leverages feature maps `featmap_names`, with `output_size=14` and `sampling_ratio=2` to align and resize each ROI proposal from the RPN to the optimal feature map in the feature map hierarchy.   
    2. **MaskRCNNHeads**: `N` layers of `Conv3dNormActivation(channels_out=256, kernel_size=3, padding=1)` layers
    3. **MaskRCNNPredictor**:  
        - `ConvTranspose3d(channel_in=256, channels_out=256, kernel_size=2, stride=2)` 
        - `Relu`
        - `Conv3d(channel_in=256, channels_out=num_classes, kernel_size=1, stride=1)`  

#### MASK DINO Overview

Adapted from IDEA-Research's `MASK DINO` (MASK DETR with Improved Denoising Anchor Boxes), our `3D-MASK DINO` replaces all 2D operations with their 3D counterparts, utilizing flash deformable attention kernels that can be found in the `ops3d` submodule.

### References

[1]: “Deep Residual Learning for Image Recognition,” (https://arxiv.org/abs/1512.03385)  

[2]: “Bag of Tricks for Image Classification with Convolutional Neural Networks,” (https://arxiv.org/abs/1812.01187)

[3]: “Exploring Plain Vision Transformer Backbones for Object Detection,” (https://arxiv.org/abs/2203.16527)  

[4]: “Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles,” (https://arxiv.org/abs/2306.00989)

[5]: “Feature Pyramid Networks for Object Detection,” (https://arxiv.org/abs/1612.03144)

[6]: “Faster R‑CNN: Towards Real-Time Object Detection with Region Proposal Networks,” (https://arxiv.org/abs/1506.01497)  

[7]: “Mask R‑CNN,” (https://arxiv.org/abs/1703.06870).


## Evaluators

**`DatasetEvaluator:`** Abstract base class defining three key methods: `reset()` for initialization, `process()` for accumulating predictions and ground truth during inference, and `evaluate()` for computing final metrics and returning results as dictionaries.

**`SkittlezInstanceEvaluator:`** Concrete implementation for instance segmentation evaluation that processes predicted vs ground truth masks through configurable metrics, merges instance masks into binary or logit representations, and returns aggregated performance scores.

## Acknowledgements
Many of our abstractions are extended from `Detectron2`, `Torchvision`, `MMEngine`, and other open source libraries. We thank the authors of these libraries for their contributions to the field of computer vision.